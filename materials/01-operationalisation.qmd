---
title: "Operationalisating Variables"
output: html_document
date: "2023-05-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

This section of the course covers how we define and measure variables, and how that can affect our analyses. This is illustrated with an example dataset. 

## Exercise 1 - Cycling to work

For this example, we're interested in finding out whether cycling to work increases staff members' productivity.

Download the productivity.csv file.

This file contains a fictional dataset that explores the relationship between cycling to work and productivity at work. Each row corresponds to a different staff member at a small Cambridge-based company. There are four variables: `cycle` is a categorical variable denoting whether the individual cycles to work; `distance` is the distance in kilometres between the individual's house and the office; `projects` is the number of projects successfully completed by the individual within the last 6 months; and `mean_hrs` is the average number of hours worked per week in the last 6 months.

As you may have noticed, we have two variables here that could serve as measures of productivity, and two ways of looking at cycling - whether someone cycles, versus how far they cycle.

First, let's start by reading in the data, and visualising it.

```{R, results=-1, message=FALSE}
productivity <- read_csv("data/productivity.csv")

head(productivity)
```

Now it's time to explore this data in a bit more detail. We can gain some insight by examining our two measures of "cycling" (our yes/no categorical variable, and the distance between home and office) and our two measures of "productivity" (mean hours worked per week, and projects completed in the last 6 months). 

```{R}
# visualise using a boxplot

productivity %>%
  ggplot(aes(x = cycle, y = distance)) +
  geom_boxplot()

# compare distance between those who cycle vs those who don't
# NB: we use a t-test here, since there are only two groups

t.test(distance ~ cycle, data = productivity)

productivity %>%
  ggplot(aes(x = as.factor(projects), y = mean_hrs)) +
  geom_boxplot()

# construct a one-way ANOVA, treating projects as a categorical variable

lm_1 <- lm(mean_hrs ~ as.factor(projects), data = productivity)
anova(lm_1)
```

What does this tell you about these two sets of variables, which (in theory at least!) tap into the same underlying construct, relate to one another? Can you spot any problems, or have you got any concerns at this stage?

If so, hold that thought.

The next step is to run some exploratory analyses. Since we're not going to reporting these data in any kind of paper or article, and the whole point is to look at different versions of the same analysis with different variables, we won't worry about multiple comparison correction this time.

When treating `mean_hrs` as our response variable, we can use standard linear models approach, since this variable is continous.

```{R}
# visualise using ggplot

productivity %>%
  ggplot(aes(x = cycle, y = mean_hrs)) +
  geom_boxplot()

# run a t-test to compare mean_hrs for those who cycle vs those who don't

t.test(mean_hrs ~ cycle, data = productivity)

productivity %>%
  ggplot(aes(x = distance, y = mean_hrs)) +
  geom_point()

# run a simple linear regression analysis

lm_2 <- lm(mean_hrs ~ distance, data = productivity)
anova(lm_2)
```

This shows us that while `cycle` does not significantly predict `mean_hrs`, `distance` does. (If you had some concerns about the `distance` variable earlier, continue to hold that thought.)

When treating `projects` as our response variable, we now have to use a GLM - specifically, we'll use Poisson regression, since `projects` is a count variable. If you aren't familiar with GLMs or Poisson regression, don't worry - the output is very similar to a linear model!

```{R}
productivity %>%
  ggplot(aes(x = distance, y = projects)) +
  geom_point()

glm_1 <- glm(projects ~ distance, data = productivity,  family = "poisson")
summary(glm_1)

productivity %>%
  ggplot(aes(x = cycle, y = projects)) +
  geom_boxplot()

glm_2 <- glm(projects ~ cycle, data = productivity, family = "poisson")
summary(glm_2)
```

This shows us that while `cycle` does significantly predict `projects`, `distance` does not (although it's only marginally non-significant). This is the opposite pattern, more or less, to the one we had for `mean_hrs`.

#### That thought you were holding...

Those of you who are discerning may have noticed that the `distance` variable is problematic as a measure of "cycling to work" in this particular dataset - this is because the dataset includes all the distances to work for the staff members who *don't* cycle, as well as those who do.

What happens if we remove those values, and look at the relationship between `distance` and our response variables again?

```{R}
# use the filter function to retain only the rows where the staff member cycles

productivity_cycle <- productivity %>%
  filter(cycle == "yes")
```

We'll repeat earlier visualisations and analyses, this time with the colour aesthetic helping us to visualise how the `cycle` variable affects the relationships between `distance`, `mean_hrs` and `projects`.

```{R}
productivity %>%
  ggplot(aes(x = distance, y = mean_hrs, colour = cycle)) +
  geom_point()

lm_3 <- lm(mean_hrs ~ distance, data = productivity_cycle)
anova(lm_3)

productivity %>%
  ggplot(aes(x = distance, y = projects, colour = cycle)) +
  geom_point()

glm_3 <- glm(projects ~ distance, data = productivity_cycle,  family = "poisson")
summary(glm_3)
```

Ah. Turns out we were right to be concerned; when staff members who don't cycle are removed from the dataset, the significant relationship that we saw earlier between `distance` and `mean_hrs` disappears. And the marginally non-significant relationship we observed between `distance` and `projects` becomes much less significant.

This leaves us with just one significant result: `projects ~ cycle`. But if we really were trying to report on these data, in a paper or report of some kind, we'd need to think very carefully about how much we *trust* this result, or whether perhaps we've stumbled on a false positive by virtue of running so many tests. We may also want to think carefully about whether or not we're happy with these definitions of the variables; for instance, is the number of projects completed really the best metric for productivity at work?

## Summary

::: {.callout-tip}
#### Key Points

- There are multiple ways to operationalise a variable, which may affect whether the variable is categorical or continuous
- The nature of the response variable will alter what type of model can be fitted to the dataset
- Some operationalisations may better capture your variable of interest than others
- If you do not effectively operationalise your variable in advance, you may find yourself "cherry-picking" your dataset
:::