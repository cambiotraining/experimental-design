[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experimental Design",
    "section": "",
    "text": "Overview\nThis one-day course is designed to complement training in statistical analysis, and focuses on how to design effective experiments while bearing in mind the planned analysis.\nIncluded topics:",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Experimental Design",
    "section": "",
    "text": "Setting a good research question\nChoosing & defining variables\nConfounding variables\nIndependence & pseudoreplication\nRevisiting statistical power\nCase study examples, for discussion\n\n\n\n\n\n\n\nLearning Objectives\n\n\n\n\nFeel confident designing experiments with statistical analysis in mind\nUnderstand common “pitfalls” that occur when designing experiments, and how to avoid or combat them\nApply these skills to at least one case study example\n\n\n\n\nPrerequisites\nKnowledge of core statistical concepts, including the statistical inference framework, linear modelling and power analysis, are required for the course. We recommend that students have attended the Core Statistics course or an equivalent.\nSome of the course materials have been created using R; users may wish to follow along by copying the code themselves. If so, knowledge of statistical analysis in R is preferred.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Experimental Design",
    "section": "Authors",
    "text": "Authors\n\nAbout the authors:\n\nVicki Hodgson\nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing - original draft; conceptualisation; coding; creation of synthetic datasets\nMartin van Rongen  \nAffiliation: Bioinformatics Training Facility, University of Cambridge\nRoles: writing; conceptualisation; coding",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Data & Setup",
    "section": "",
    "text": "Data\nThe data used in these materials is provided as a zip file. Download and unzip the folder to your Desktop to follow along with the materials.\nDownload\nThese files contain synthetic datasets that have been created specifically for the course.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#setup",
    "href": "setup.html#setup",
    "title": "Data & Setup",
    "section": "Setup",
    "text": "Setup\n\nR and RStudio\n\n\nWindows\nDownload and install all these using default options:\n\nR\nRTools\nRStudio\n\n\n\nMac OS\nDownload and install all these using default options:\n\nR\nRStudio\n\n\n\nLinux\n\nGo to the R installation folder and look at the instructions for your distribution.\nDownload the RStudio installer for your distribution and install it using your package manager.\n\n\n\n\nPackages\nThe course materials require the tidyverse package.\nYou can download and activate the package using the following commands:\ninstall.packages(\"tidyverse\")\n\nlibrary(tidyverse)",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "materials/01-research-questions.html",
    "href": "materials/01-research-questions.html",
    "title": "3  The Research Question",
    "section": "",
    "text": "3.1 Criteria for a good research question\nThis section of the course talks in more detail about what constitutes a good research question. Formulating a research question is an early and important step when designing experiments - though sometimes it can be tempting to dive right in without narrowing down what you want to research, this will make it impossible to plan your analysis in advance, and increases your chances of falling into common pitfalls.\nYour research question serves several important purposes:\nFocused on a single topic\nThe scope of the question is important. If your research question is too broad, or tries to tackle multiple topics at once, you’ll struggle to design an experiment that can actually answer it. Even if you succeed, your experiment will necessarily be more complicated and take more time and resources to conduct, and may require compromising on some of your\nUses specific, well-defined concepts\nOr, alternatively, be prepared with your own definition of the concepts/variables that you’re studying! It should be clear to you in advance of collecting data precisely what your variables of interest are and how you’re going to measure them. This should also be clear to other researchers who may be reading your work or using your data, since your experiment should also be repeatable by others. This is all particularly true if you’re studying something that is a little abstract or can be defined in multiple ways. (See the section on operationalising variables for more on this.)\nRelevant and addressing a current research need\nYour research question should be focused on something that is of use to the scientific community or the public more broadly, either by increasing our understanding of basic science or by promoting translation to clinical or industry settings. It should, ideally, be motivated by the existing literature, or more specifically, by gaps or outstanding questions in the existing literature. We could get very philosophical here about the role of researchers in society, but the focus of this course is much more practical than that, and from a practical perspective (and a cynical one): these are the studies that get funded and published!\nResearchable\nThis perhaps seems obvious, but let’s unpack it a bit. It should be possible to answer your research question either by collecting original data, or using credible existing sources (e.g., a meta-analysis), and crucially, your research question shouldn’t depend on any subjective opinions or value judgements. For instance, it’s a good idea to avoid words like “best” or “worst” in your questions.\nOriginal\nIn short: what you’re trying to find out, shouldn’t be possible to find out elsewhere already. An important caveat here is that this does not mean that attempts to replicate prior experiments aren’t valid - they absolutely are. The research question for a replication study is different to the earlier study, because it is specifically asking whether a previously observed effect or phenomenon can be replicated.\nComplex and insightful\nBy “complex”, we don’t necessarily mean that the research question has to be difficult or convoluted for the sake of it - but it should have more than a simple yes or no answer. There are situations where a yes or no answer forms part of your overall conclusion - e.g., can tardigrades survive in the vacuum of space? Will compound X react with compound Y? Can humans remember their dreams? - but in all of these situations, a well-designed experiment should also give additional information, like the degree to which a particular phenomenon is observed, or details about the conditions under which it occurs. If you’re going to the effort of designing and conducting an experiment, you may as well be getting some detailed insight!",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Research Question</span>"
    ]
  },
  {
    "objectID": "materials/01-research-questions.html#some-examples",
    "href": "materials/01-research-questions.html#some-examples",
    "title": "3  The Research Question",
    "section": "3.2 Some examples",
    "text": "3.2 Some examples\nBelow are a list of research questions that don’t quite meet all of the above criteria. Have a read through them, and see if you can identify the issues, and how you might refine the question to improve it.\n\nDoes owning a dog make you more likeable?\nWhy do programmers make typos in their code?\nHow does nuclear radiation affect humans?\nDo human beings have a soul that continues to exist after death?\nIs bureaucracy in the University bad or good?\nDoes spending a lot of time on social media affect children’s development?\nCan pet parrots live more than 50 years?",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Research Question</span>"
    ]
  },
  {
    "objectID": "materials/01-research-questions.html#summary",
    "href": "materials/01-research-questions.html#summary",
    "title": "3  The Research Question",
    "section": "3.3 Summary",
    "text": "3.3 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nWithin the statistical inference and hypothesis testing framework, it is essential to set a good research question\nSetting the question is the first step to designing the experiment that will answer it\nA good research question should be focused, researchable, relevant, feasible, original and complex",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Research Question</span>"
    ]
  },
  {
    "objectID": "materials/02-operationalisation.html",
    "href": "materials/02-operationalisation.html",
    "title": "4  Operationalising Variables",
    "section": "",
    "text": "4.1 Libraries and functions\nThis section of the course covers how we define and measure variables, and how that can affect our analyses. This is illustrated with an example dataset. If you want to do the exercises yourself, make sure to check if you have all the required libraries installed.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Operationalising Variables</span>"
    ]
  },
  {
    "objectID": "materials/02-operationalisation.html#libraries-and-functions",
    "href": "materials/02-operationalisation.html#libraries-and-functions",
    "title": "4  Operationalising Variables",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n4.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n\n\n\n\n4.1.2 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Operationalising Variables</span>"
    ]
  },
  {
    "objectID": "materials/02-operationalisation.html#exercise-1---cycling-to-work",
    "href": "materials/02-operationalisation.html#exercise-1---cycling-to-work",
    "title": "4  Operationalising Variables",
    "section": "4.2 Exercise 1 - Cycling to work",
    "text": "4.2 Exercise 1 - Cycling to work\nFor this example, we’re interested in finding out whether cycling to work increases staff members’ productivity.\nDownload the productivity.csv file.\nThis file contains a fictional dataset that explores the relationship between cycling to work and productivity at work. Each row corresponds to a different staff member at a small Cambridge-based company. There are four variables: cycle is a categorical variable denoting whether the individual cycles to work; distance is the distance in kilometres between the individual’s house and the office; projects is the number of projects successfully completed by the individual within the last 6 months; and mean_hrs is the average number of hours worked per week in the last 6 months.\nAs you may have noticed, we have two variables here that could serve as measures of productivity, and two ways of looking at cycling - whether someone cycles, versus how far they cycle.\nFirst, let’s start by reading in the data, and visualising it.\n\nRPython\n\n\n\n# load the data\nproductivity &lt;- read_csv(\"data/productivity.csv\")\n\n# and have a look\nhead(productivity)\n\n\n\n\n# load the data\nproductivity_py = pd.read_csv(\"data/productivity.csv\")\n\n# and have a look\nproductivity_py.head()\n\n  cycle  distance  projects  mean_hrs\n0    no      6.68         2      55.0\n1    no      4.32         3      48.0\n2    no      5.81         2      42.0\n3    no      8.49         2      37.5\n4    no      6.47         4      32.0\n\n\n\n\n\nNow it’s time to explore this data in a bit more detail. We can gain some insight by examining our two measures of “cycling” (our yes/no categorical variable, and the distance between home and office) and our two measures of “productivity” (mean hours worked per week, and projects completed in the last 6 months).\n\nRPython\n\n\n\n# visualise using a boxplot\n\nproductivity %&gt;%\n  ggplot(aes(x = cycle, y = distance)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# compare distance between those who cycle vs those who don't\n# NB: we use a t-test here, since there are only two groups\n\nt.test(distance ~ cycle, data = productivity)\n\n\n    Welch Two Sample t-test\n\ndata:  distance by cycle\nt = 3.4417, df = 10.517, p-value = 0.005871\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n 1.801439 8.293561\nsample estimates:\n mean in group no mean in group yes \n           7.3700            2.3225 \n\n\n\n\n\n# visualise using a boxplot\n(ggplot(productivity_py,\n        aes(x = \"cycle\",\n            y = \"distance\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\nNext, we compare the distance between those who cycle and those who do not. We use a t-test, since there are only two groups.\nHere we use the ttest() function from the pingouin library. This needs two vectors as input, so we split the data as follows and then run the test:\n\ndist_no_cycle = productivity_py.query('cycle == \"no\"')[\"distance\"]\ndist_yes_cycle = productivity_py.query('cycle == \"yes\"')[\"distance\"]\n\npg.ttest(dist_no_cycle, dist_yes_cycle).transpose()\n\n                  T-test\nT               3.441734\ndof            10.516733\nalternative    two-sided\np-val           0.005871\nCI95%        [1.8, 8.29]\ncohen-d         1.683946\nBF10              15.278\npower           0.960302\n\n\n\n\n\nLet’s look at the second set of variables: the mean hours of worked per week and the number of projects completed in the past 6 months. When visualising this, we need to consider the projects as a categorical variable.\n\nRPython\n\n\n\n# visualise the data\nproductivity %&gt;%\n  ggplot(aes(x = as.factor(projects), y = mean_hrs)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# construct a one-way ANOVA, treating projects as a categorical variable\n\nlm_1 &lt;- lm(mean_hrs ~ as.factor(projects), data = productivity)\nanova(lm_1)\n\nAnalysis of Variance Table\n\nResponse: mean_hrs\n                    Df  Sum Sq Mean Sq F value Pr(&gt;F)\nas.factor(projects)  7  936.02  133.72   1.598 0.2066\nResiduals           16 1338.88   83.68               \n\n\n\n\n\n# visualise using a boxplot\n(ggplot(productivity_py,\n        aes(x = productivity_py['projects'].astype('category'),\n            y = \"mean_hrs\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n# construct a one-way ANOVA, treating projects as a categorical variable\npg.anova(dv = \"mean_hrs\",\n         between = \"projects\",\n         data = productivity_py,\n         detailed = True).round(3)\n\n     Source        SS  DF       MS      F  p-unc    np2\n0  projects   936.023   7  133.718  1.598  0.207  0.411\n1    Within  1338.883  16   83.680    NaN    NaN    NaN\n\n\n\n\n\nWhat does this tell you about these two sets of variables, which (in theory at least!) tap into the same underlying construct, relate to one another? Can you spot any problems, or have you got any concerns at this stage?\nIf so, hold that thought.\n\nAssessing the effect of cycling on productivity\nThe next step is to run some exploratory analyses. Since we’re not going to reporting these data in any kind of paper or article, and the whole point is to look at different versions of the same analysis with different variables, we won’t worry about multiple comparison correction this time.\nWhen treating mean_hrs as our response variable, we can use standard linear models approach, since this variable is continuous.\n\nRPython\n\n\n\n# visualise using ggplot\n\nproductivity %&gt;%\n  ggplot(aes(x = cycle, y = mean_hrs)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# run a t-test to compare mean_hrs for those who cycle vs those who don't\n\nt.test(mean_hrs ~ cycle, data = productivity)\n\n\n    Welch Two Sample t-test\n\ndata:  mean_hrs by cycle\nt = -0.51454, df = 11.553, p-value = 0.6166\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -12.803463   7.928463\nsample estimates:\n mean in group no mean in group yes \n          36.6875           39.1250 \n\n\n\n\n\n# visualise using a boxplot\n(ggplot(productivity_py,\n        aes(x = \"cycle\",\n            y = \"mean_hrs\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n# run a t-test to compare mean_hrs for those who cycle vs those who don't\nhrs_no_cycle = productivity_py.query('cycle == \"no\"')[\"mean_hrs\"]\nhrs_yes_cycle = productivity_py.query('cycle == \"yes\"')[\"mean_hrs\"]\n\npg.ttest(hrs_no_cycle, hrs_yes_cycle).transpose()\n\n                    T-test\nT                -0.514542\ndof              11.553188\nalternative      two-sided\np-val             0.616573\nCI95%        [-12.8, 7.93]\ncohen-d            0.24139\nBF10                 0.427\npower             0.083194\n\n\n\n\n\nLet’s also look at mean_hrs vs distance:\n\nRPython\n\n\n\nproductivity %&gt;%\n  ggplot(aes(x = distance, y = mean_hrs)) +\n  geom_point()\n\n\n\n\n\n\n\n# run a simple linear regression analysis\n\nlm_2 &lt;- lm(mean_hrs ~ distance, data = productivity)\nanova(lm_2)\n\nAnalysis of Variance Table\n\nResponse: mean_hrs\n          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ndistance   1  473.28  473.28  5.7793 0.02508 *\nResiduals 22 1801.63   81.89                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"mean_hrs\")) +\n     geom_point())\n\n\n\n\n\n\n\n\nWe can perform a linear regression on these data:\n\n# create a linear model\nmodel = smf.ols(formula = \"mean_hrs ~ distance\",\n                data = productivity_py)\n# and get the fitted parameters of the model\nlm_productivity_py = model.fit()\n\n# look at the model output\nprint(lm_productivity_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               mean_hrs   R-squared:                       0.208\nModel:                            OLS   Adj. R-squared:                  0.172\nMethod:                 Least Squares   F-statistic:                     5.779\nDate:                Wed, 02 Aug 2023   Prob (F-statistic):             0.0251\nTime:                        16:15:28   Log-Likelihood:                -85.875\nNo. Observations:                  24   AIC:                             175.8\nDf Residuals:                      22   BIC:                             178.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     43.0833      2.711     15.891      0.000      37.461      48.706\ndistance      -1.1912      0.496     -2.404      0.025      -2.219      -0.164\n==============================================================================\nOmnibus:                        1.267   Durbin-Watson:                   1.984\nProb(Omnibus):                  0.531   Jarque-Bera (JB):                0.300\nSkew:                          -0.163   Prob(JB):                        0.861\nKurtosis:                       3.441   Cond. No.                         8.18\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nThis shows us that while cycle does not significantly predict mean_hrs, distance does. (If you had some concerns about the distance variable earlier, continue to hold that thought.)\nWhen treating projects as our response variable, we now have to use a GLM - specifically, we’ll use Poisson regression, since projects is a count variable. If you aren’t familiar with GLMs or Poisson regression, you can expand the box below to find out a bit more (including a link to further materials that will give you more detail).\n\n\n\n\n\n\nGeneralised linear models and Poisson regression\n\n\n\n\n\nStandard linear models require that your response or outcome variable be continuous. However, your variable might instead be a probability (e.g., a coin flip, or a proportion), or a count variable, which follow a binomial or Poisson distribution respectively (rather than a normal/Gaussian distribution). To account for this, generalised linear models allow the fitted linear model to be related to the outcome variable via some link function, commonly a log or logit function. Model parameters are also estimated slightly differently; as opposed to the ordinary least squares approach we use in linear regression, GLMs make use of something called maximum likelihood estimation.\nPoisson regression is a specific type of GLM, which uses a log function; it’s also sometimes referred to as a log-linear model. We use Poisson regression in scenarios where we have an outcome variable that is count data, i.e., data that only takes non-negative integer values, or when modelling contingency tables.\nIf you’d like to read more or learn how to fit GLMs yourself, you can find additional course materials here.\n\n\n\nIf GLMs don’t sound interesting to you right now, then don’t worry - the output is very similar to your typical linear model!\nFirst, we look at distance vs projects.\n\nRPython\n\n\n\nproductivity %&gt;%\n  ggplot(aes(x = distance, y = projects)) +\n  geom_point()\n\n\n\n\n\n\n\nglm_1 &lt;- glm(projects ~ distance, data = productivity,\n             family = \"poisson\")\nsummary(glm_1)\n\n\nCall:\nglm(formula = projects ~ distance, family = \"poisson\", data = productivity)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.54068    0.15119  10.190   &lt;2e-16 ***\ndistance    -0.06038    0.03301  -1.829   0.0674 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 23.485  on 23  degrees of freedom\nResidual deviance: 19.766  on 22  degrees of freedom\nAIC: 97.574\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"projects\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n# create a generalised linear model\nmodel = smf.poisson(formula = \"projects ~ distance\",\n                    data = productivity_py)\n# and get the fitted parameters of the model\nglm1_py = model.fit()\n\nOptimization terminated successfully.\n         Current function value: 1.949462\n         Iterations 5\n\n# look at the model output\nprint(glm1_py.summary())\n\n                          Poisson Regression Results                          \n==============================================================================\nDep. Variable:               projects   No. Observations:                   24\nModel:                        Poisson   Df Residuals:                       22\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 02 Aug 2023   Pseudo R-squ.:                 0.03822\nTime:                        16:15:30   Log-Likelihood:                -46.787\nconverged:                       True   LL-Null:                       -48.647\nCovariance Type:            nonrobust   LLR p-value:                   0.05380\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.5407      0.151     10.190      0.000       1.244       1.837\ndistance      -0.0604      0.033     -1.829      0.067      -0.125       0.004\n==============================================================================\n\n\n\n\n\nNext, we look at cycle vs projects.\n\nRPython\n\n\n\nproductivity %&gt;%\n  ggplot(aes(x = cycle, y = projects)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nglm_2 &lt;- glm(projects ~ cycle, data = productivity,\n             family = \"poisson\")\nsummary(glm_2)\n\n\nCall:\nglm(formula = projects ~ cycle, family = \"poisson\", data = productivity)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.9163     0.2236   4.098 4.17e-05 ***\ncycleyes      0.5596     0.2535   2.207   0.0273 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 23.485  on 23  degrees of freedom\nResidual deviance: 18.123  on 22  degrees of freedom\nAIC: 95.931\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"cycle\",\n            y = \"projects\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n# create a generalised linear model\nmodel = smf.poisson(formula = \"projects ~ cycle\",\n                    data = productivity_py)\n# and get the fitted parameters of the model\nglm2_py = model.fit()\n\nOptimization terminated successfully.\n         Current function value: 1.915222\n         Iterations 5\n\n# look at the model output\nprint(glm2_py.summary())\n\n                          Poisson Regression Results                          \n==============================================================================\nDep. Variable:               projects   No. Observations:                   24\nModel:                        Poisson   Df Residuals:                       22\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 02 Aug 2023   Pseudo R-squ.:                 0.05512\nTime:                        16:15:33   Log-Likelihood:                -45.965\nconverged:                       True   LL-Null:                       -48.647\nCovariance Type:            nonrobust   LLR p-value:                   0.02057\n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept        0.9163      0.224      4.098      0.000       0.478       1.355\ncycle[T.yes]     0.5596      0.254      2.207      0.027       0.063       1.057\n================================================================================\n\n\n\n\n\nThis shows us that cycle significantly predicts projects, meaning the number of projects that get completed is not completely random, but some of the variance in that can be explained by whether a person cycles to work, or not. In contrast, distance does not appear to be a significant predictor of projects (although it’s only marginally non-significant). This is the opposite pattern, more or less, to the one we had for mean_hrs.\n\n\nThat thought you were holding…\nThose of you who are discerning may have noticed that the distance variable is problematic as a measure of “cycling to work” in this particular dataset - this is because the dataset includes all the distances to work for the staff members who don’t cycle, as well as those who do.\nWhat happens if we remove those values, and look at the relationship between distance and our response variables again?\n\nRPython\n\n\n\n# use the filter function to retain only the rows where the staff member cycles\n\nproductivity_cycle &lt;- productivity %&gt;%\n  filter(cycle == \"yes\")\n\n\n\n\nproductivity_cycle_py = productivity_py[productivity_py[\"cycle\"] == \"yes\"]\n\n\n\n\nWe’ll repeat earlier visualisations and analyses, this time with the colour aesthetic helping us to visualise how the cycle variable affects the relationships between distance, mean_hrs and projects.\n\nRPython\n\n\n\nproductivity %&gt;%\n  ggplot(aes(x = distance, y = mean_hrs, colour = cycle)) +\n  geom_point()\n\n\n\n\n\n\n\nlm_3 &lt;- lm(mean_hrs ~ distance, data = productivity_cycle)\nanova(lm_3)\n\nAnalysis of Variance Table\n\nResponse: mean_hrs\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)\ndistance   1  202.77 202.766  2.6188 0.1279\nResiduals 14 1083.98  77.427               \n\nproductivity %&gt;%\n  ggplot(aes(x = distance, y = projects, colour = cycle)) +\n  geom_point()\n\n\n\n\n\n\n\nglm_3 &lt;- glm(projects ~ distance, data = productivity_cycle,\n             family = \"poisson\")\nsummary(glm_3)\n\n\nCall:\nglm(formula = projects ~ distance, family = \"poisson\", data = productivity_cycle)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.55102    0.16292    9.52   &lt;2e-16 ***\ndistance    -0.03380    0.05204   -0.65    0.516    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 15.591  on 15  degrees of freedom\nResidual deviance: 15.144  on 14  degrees of freedom\nAIC: 70.869\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"mean_hrs\",\n            colour = \"cycle\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"mean_hrs ~ distance\",\n                data = productivity_cycle_py)\n# and get the fitted parameters of the model\nlm_dist_cycle_py = model.fit()\n\n# look at the model output\nprint(lm_dist_cycle_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               mean_hrs   R-squared:                       0.158\nModel:                            OLS   Adj. R-squared:                  0.097\nMethod:                 Least Squares   F-statistic:                     2.619\nDate:                Wed, 02 Aug 2023   Prob (F-statistic):              0.128\nTime:                        16:15:37   Log-Likelihood:                -56.429\nNo. Observations:                  16   AIC:                             116.9\nDf Residuals:                      14   BIC:                             118.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     42.4204      2.998     14.151      0.000      35.991      48.850\ndistance      -1.4189      0.877     -1.618      0.128      -3.299       0.462\n==============================================================================\nOmnibus:                        4.148   Durbin-Watson:                   2.906\nProb(Omnibus):                  0.126   Jarque-Bera (JB):                2.008\nSkew:                          -0.820   Prob(JB):                        0.366\nKurtosis:                       3.565   Cond. No.                         4.85\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# visualise using a scatterplot\n(ggplot(productivity_py,\n        aes(x = \"distance\",\n            y = \"projects\",\n            colour = \"cycle\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n# create a poisson model\nmodel = smf.poisson(formula = \"projects ~ distance\",\n                    data = productivity_cycle_py)\n# and get the fitted parameters of the model\nlm_proj_cycle_py = model.fit()\n\nOptimization terminated successfully.\n         Current function value: 2.089649\n         Iterations 4\n\n# look at the model output\nprint(lm_proj_cycle_py.summary())\n\n                          Poisson Regression Results                          \n==============================================================================\nDep. Variable:               projects   No. Observations:                   16\nModel:                        Poisson   Df Residuals:                       14\nMethod:                           MLE   Df Model:                            1\nDate:                Wed, 02 Aug 2023   Pseudo R-squ.:                0.006655\nTime:                        16:15:40   Log-Likelihood:                -33.434\nconverged:                       True   LL-Null:                       -33.658\nCovariance Type:            nonrobust   LLR p-value:                    0.5033\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      1.5510      0.163      9.520      0.000       1.232       1.870\ndistance      -0.0338      0.052     -0.650      0.516      -0.136       0.068\n==============================================================================\n\n\n\n\n\nAh. Turns out we were right to be concerned; when staff members who don’t cycle are removed from the dataset, the significant relationship that we saw earlier between distance and mean_hrs disappears. And the marginally non-significant relationship we observed between distance and projects becomes much less significant.\nThis leaves us with just one significant result: projects ~ cycle. But if we really were trying to report on these data, in a paper or report of some kind, we’d need to think very carefully about how much we trust this result, or whether perhaps we’ve stumbled on a false positive by virtue of running so many tests. We may also want to think carefully about whether or not we’re happy with these definitions of the variables; for instance, is the number of projects completed really the best metric for productivity at work?",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Operationalising Variables</span>"
    ]
  },
  {
    "objectID": "materials/02-operationalisation.html#summary",
    "href": "materials/02-operationalisation.html#summary",
    "title": "4  Operationalising Variables",
    "section": "4.3 Summary",
    "text": "4.3 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nThere are multiple ways to operationalise a variable, which may affect whether the variable is categorical or continuous\nThe nature of the response variable will alter what type of model can be fitted to the dataset\nSome operationalisations may better capture your variable of interest than others\nIf you do not effectively operationalise your variable in advance, you may find yourself “cherry-picking” your dataset",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Operationalising Variables</span>"
    ]
  },
  {
    "objectID": "materials/03-confounds.html",
    "href": "materials/03-confounds.html",
    "title": "5  Confounds and Bias",
    "section": "",
    "text": "5.1 What is a confounding variable?\nThis section of the course talks in more detail about how to deal with confounding variables in research.\nA confound is a variable that covaries with both your predictor(s) and outcome variables, such that the relationship between the predictor(s) and outcomes is affected. Confounding variables (sometimes also referred to as extraneous variables) are often controlled for in research, wherever it’s possible to do so (in which case, you may also see them called control variables).\nLet’s have a look at this visually, with an example, because that’s easier to wrap your head around!\nA confound can obscure an effect of interest, as in the example below. When looking at the effect of soil type alone, it would appear that the difference between the two group means is very small, compared to the spread within the groups (remember that a t-test, along with other linear models, is more or less a signal-to-noise ratio!). However, we can see from the graph on the right that a great deal of this variation within the two groups is due to the effect of sunlight. In this sample, sunlight is a confounding variable that has contributed additional variance to our outcome response. When we take sunlight into account and eliminate some of this additional variance, we now see a consistent pattern - at all levels of sunlight, there is a higher rate of growth for soil A than soil B (split above and below the line of best fit).\nA confound may also make it appear than the variable of interest has an effect, when in reality it does not. Consider an alternative version of the same example, below. Here, when we look at soil type alone, we see what seems to be quite a large effect, with quite a big difference between the group means relative to the spread within each group. However, when we also consider the effect of sunlight, it becomes clear that the effect of soil is artificial. Sunlight has varied systematically with soil type in this sample, such that plants in soil A have received notably more sunlight than soil B. The effect of soil type here is minimal, if it exists at all.\nFrom both of these examples, we can see that the reason we need to worry about confounds is that they can cause us to misattribute variance in our analysis - making us over- or under-estimate the variance explained by one or more of our predictor(s). Modelling data is all about trying to correctly interpret the noise and variation that we see in our datasets, to try to find meaningful relationships between variables that hold true not only in our sample, but in the underlying population we’ve drawn from. So, there’s no wonder we worry about confounds!",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Confounds and Bias</span>"
    ]
  },
  {
    "objectID": "materials/03-confounds.html#what-is-a-confounding-variable",
    "href": "materials/03-confounds.html#what-is-a-confounding-variable",
    "title": "5  Confounds and Bias",
    "section": "",
    "text": "The effect of sunlight has obscured the effect of soil type\n\n\n\n\n\n\nThe effect of sunlight has created an “artificial” effect of soil type",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Confounds and Bias</span>"
    ]
  },
  {
    "objectID": "materials/03-confounds.html#sources-of-confounds",
    "href": "materials/03-confounds.html#sources-of-confounds",
    "title": "5  Confounds and Bias",
    "section": "5.2 Sources of confounds",
    "text": "5.2 Sources of confounds\nConfounds and variation can come from a variety of sources, including:\n\nBatch effects\nOrder effects\nSystematic measurement or technical error\nDemand characteristics & placebo effect\nBiased sampling & assignment to conditions\nExperimenter confounds\nEnvironmental confounds\n\nEach of these is discussed in more detail below.\n\nBatch effects\nBatch effects refers to the variation that can occur between repeats or observations, usually introduced by equipment or technical factors. The most common example of batch effects in classical lab work are plate effects. Samples are grouped together in well plates (most commonly a 96-well plate), and there is often a great deal of variation between plates due to differences in the way they are handled and processed. (Within plates, there are also established row, column and edge effects, which can complicate matters even further!) Sometimes, these differences are larger than the differences observed between experimental conditions.\nFor those who find bench work less relatable: another example of batch effects is growing plants across multiple greenhouses for the same study. There may be systematic differences between greenhouses - perhaps some are south-facing, and others are east-facing; perhaps they are looked after by different research assistants, who aren’t consistent in the amount of water they give the plants. And within the greenhouses, plants near the edges will receive more light and have more space to branch out than those near the centre.\n\n\nOrder effects\nOrder effects may occur in repeated measures designs - i.e., when observations in your sample undergo more than one of the experimental conditions - and the order of conditions or treatments alters the effect those conditions have on the outcome variable.\nSometimes, this is because the conditions have unexpected interactions with one another; for instance, if you are testing the efficacy of two different drugs, but drug A blocks the effects of drug B, leading you to believe that drug B doesn’t work when in fact it could. Other times, it may simply be because participants become fatigued, and their performance declines over time (a particular problem if you’re asking people or animals to perform cognitive tasks like memory or puzzle solving), or the opposite might happen - participants might get better with practice.\n\n\nSystematic measurement or technical error\nThis type of error may be generated by the researcher(s), for instance, if a batch of media for cell culture has been prepared incorrectly, or the wrong dosage of a drug has been given. Alternatively, equipment itself can be systematically incorrect or inconsistent, in a way that may covary with variables of interest. In functional MRI studies, for instance, researchers must contend with a phenomenon called “scanner drift”: a gradual decrease in signal intensity over the course of the scan.\nThis type of confound can be a particular problem in multi-site studies, e.g., clinical studies taking place across multiple hospitals, where the model and calibration of equipment is likely to differ.\n\n\nDemand characteristics\nThis is a term borrowed from psychology, in which this type of confound is particularly prevalent. Demand characteristics refers to the unintended extraneous and environmental factors that might give human participants clues about what the experiment is about, and subconsciously change their behaviour in response.\nSometimes, this means that participants will actively try to behave in a way that confirms the experimenter’s hypothesis; other, more contrary folk like to try to “ruin” or “disprove” the hypothesis by behaving in the opposite way. Sometimes, people can get so wrapped up in the anxiety of being evaluated that they stop behaving normally, viewing the experimenter as an authority figure and trying to follow instructions to the letter. (If you’re curious how far this can go, there are several social psychology studies that investigated this phenomenon, back in those crazy days from before ethics committees existed; look up the Milgram experiments.)\nIn fact, you’re probably familiar with the placebo effect, which is a well-known example of how taking part in a study can alter participants’ responses. Simply believing that you’re being given a substance or treatment of some kind can lead to real, psychosomatic responses to something that doesn’t even exist.\n\n\nBiased sampling & assignment\nA key assumption of null hypothesis significance testing is that our sample is representative of the underlying population we’re trying to learn about. But there are lots of things that might make this untrue. It’s a well-known problem in psychology studies, for instance, that human participants who sign up for studies are often quite different from the general population - they’re usually university students with an interest in psychology. In clinical research, too, you may find that only a certain subset of patients are willing or able to take part (e.g., those who are able to travel, or those who are seeking experimental treatments). This can make it hard to generalise to the population of interest.\nEven if your sample is truly randomly selected, it’s also possible to introduce confounds when assigning to different experimental conditions.\n\n\nExperimenter effects\nBeing there to observe the science sometimes means that we accidentally (or even deliberately, but that’s a whole separate discussion) affect the science.\nThis can happen in a bunch of ways. One is the observer-expectancy effect, in which researchers subconsciously influence the experiment due to their own hypotheses or biases, and is especially prevalent in studies with human participants. This might involve asking leading questions or subconsciously give hints as to how to behave, as happened with the case of the famous “counting” horse, Clever Hans (I’ve put some info about him in the drop-down box below, mostly because it’s a cool story).\nThere is also the phenomenon of confirmation bias. Confirmation bias refers to our tendency to seek out information that confirms what we’re already thinking, and disregard or minimise evidence that goes against it. You can imagine how this might be problematic in research - especially if researchers think that they’re immune to it, which sadly, we are not.\nAnd, of course, we have to consider the basic physical effects of being present as a researcher. People and animals will behave differently when there’s someone with a white lab coat in the room with them. Further, individual characteristics of the researcher may affect responses - for instance, it’s a known phenomenon that lab rats will behave differently or be stressed to different degrees when handled by male versus female researchers.\n\n\n\n\n\n\nThe Clever Hans effect\n\n\n\n\n\nHans was a horse who became famous around the turn of the 19th century in Germany for supposedly being able to count, among other skills (including working with fractions, telling time, reading calendars and even reading and understanding German). When asked a question, he would paw the ground with a hoof the appropriate number of times to give his response, and he was amazingly accurate.\nSadly, of course, Hans was not actually a genius horse performing all these tasks - he was getting his cues from his trainer and owner, Wilhelm von Osten. Hans was picking up on subtle changes in body language that let him know when to stop “counting”; this meant that he couldn’t answer questions that his owner didn’t know the answer to. Crucially, van Osten had no idea he was providing these cues at all, and truly believed his pony was just super smart.\nThe term “Clever Hans effect” has been coined in honour of the study on Hans performed by psychologist Oskar Pfungst, who uncovered what was really going on. The Clever Hans effect is an example of the observer-expectancy effect that most often crops up in studies of animal cognition.\n\n\n\n\n\nEnvironmental confounds\nThis is something of a catch-all term, and you’ll notice that lots of the types of confounds described above could also be described as “environmental” confounds. Here, though, I’m talking specifically about random and unpredictable environmental effects that can’t really be controlled for.\nWe conduct experiments in some kind of physical space, and those spaces are rarely entirely under our control. Sometimes, stuff happens! Perhaps the fridge you’re using to store some of your samples happened to malfunction and got a bit too warm overnight without you realising. Perhaps some of the soil you’re using is too high in nitrogen. Perhaps the testing room was too hot, and the participants who came in for testing on those days were grumpy.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Confounds and Bias</span>"
    ]
  },
  {
    "objectID": "materials/03-confounds.html#dealing-with-confounds",
    "href": "materials/03-confounds.html#dealing-with-confounds",
    "title": "5  Confounds and Bias",
    "section": "5.3 Dealing with confounds",
    "text": "5.3 Dealing with confounds\n\nMatched pairs designs and blocked randomisation to conditions\nThe aim of both of these methods is to assign participants or observations to experimental groups, while keeping those groups as similar to one another as possible. Often, this is to make sure that demographic variables like gender or age don’t differ systematically between groups.\nA matched pairs design would be used in situations where you as the researcher don’t have any control over how the groups are split. For instance, if you’re comparing patients to healthy controls, you don’t get to assign people to groups - the groups already exist in the environment. What is typically done in these situations is to start with the experimental or patient group, and find a control that matches each of them (for factors like age, gender, education level, and any other possible confounds). This can be quite time consuming, and often requires having a pool of controls to choose from, but it does ensure that there aren’t any systematic differences between the groups that might obscure or exaggerate a real effect of the predictor variable(s).\n\n\n\nMatched pairs design\n\n\nBlocked randomisation can be used in situations where you as the researcher do control which observations or participants are assigned to which group. The first step is to construct blocks within your sample, each of which is broadly representative of the sample as a whole (with respect to the variables that you want to control for). These blocks are then assigned randomly to conditions, which ensures that the sample is stratified.\n\n\n\nBlocked randomisation to conditions\n\n\n\n\nRandomisation\nRandomisation is a relatively broad term in experimental design - it can refer to random selection of the sample, or random allocation to conditions, which has been discussed above.\nRandomisation can be particularly useful in avoiding batch effects, and/or predictable, systematic environmental effects. It essentially involves “mixing” replicates of the different conditions across batches (whether these batches constitute well-plates, greenhouses, clinical sites and so on), rather than having only one condition per batch. This means that any environmental or experimenter variables that affect the batch, will affect replicates of all the conditions similarly. If it’s done properly, randomisation allows us to estimate the variation that’s due to batch differences, and leave behind the variation that’s due to our predictor variable(s) of interest.\nAn example of how randomisation can help to eliminate unwanted batch effects can be found in this blog post discussing the GenADA study on Alzheimer’s by GSK.\n The figure above shows a simplified version of how we might use randomisation. On the far left, we have a batch in which all replicates are from a single condition - this is something we absolutely want to avoid. Our two examples on the right are randomised, where conditions are mixed up between rows and columns; on the far right, we have a special example of such randomisation that we call a Latin square, in which each condition is replicated only once on each row and in each column (I think of it like a sudoku puzzle).\nOf course, it’s not always possible to completely intermingle your replicates like this. Sometimes, attempting to perfectly randomise every single replicate can lead to errors - lab work is a good example of this, since pipetting randomly into a well-plate can really increase the chance of experimenter error, which may be more damaging than the batch effects you were trying to avoid. As a compromise, partially randomised designs may work instead, as shown in the middle left. Although we still have potential row effects here, we have at least avoided the issue of only including one condition per batch!\n\n\nCounterbalancing\nThe primary use of counterbalancing is to prevent order effects. In a repeated measures design, the sample can be “counterbalanced” such that the conditions are presented in a different order to different subsets of that sample.\nWhen counterbalancing, your sample should be equally divided across the counterbalanced conditions/groups; further, within each experimental group, that subset of the sample should also be equally divided across the counterbalanced groups to avoid any systematic bias. This means that the assignment to counterbalanced groups ends up being pseudo-randomised.\nHere’s a visualisation to help explain what that might look like. The sample has a total n = 8, and is split into two experimental groups (let’s pretend we’re interested in the effect of pink versus green). We want each person to perform two different tasks, but we want to make sure that if they’re getting tired after the first one, or if the first task somehow biases their answers to the second, that we’re not disproportionately seeing the impact on Task B. So, we have two different possible task orders, creating two counterbalancing groups/conditions. We assign an equal number to each of those conditions, and further, we have an equal stratification of pink versus green within each of those conditions:\n\n\n\nCounterbalancing a sample across two tasks\n\n\n\n\nBlinding\nIn a single blind protocol, participants are aware which condition they have been assigned to. Of course, the majority of experiments in the life sciences are single blinded, since things like cell cultures, plants, receptors, enzymes etc. aren’t anywhere close to sentient enough to figure things out about the experiment they’re in. This technique is mainly used for studies with human participants, where the control group(s) are given a placebo or sham treatment, such that the overall experience is as close as possible to that of the experimental group.\nIn a double blind protocol, often considered to be the best choice for studies such as clinical trials, both the participant and the experimenter are blind to which condition the participant has been assigned to. In addition to eliminating placebo effects, this also means that the researcher cannot subconsciously influence the results (e.g., by treating groups differently).\nData analysis can also be blinded - i.e., the data is collected and analysed by different researchers. This helps to remove influences of confirmation bias.\n\n\nCovariates of no interest\nSometimes, there is no good way of mitigating a confound in your experimental design. All is not necessarily lost, however, because in some of those cases, you can include a confound as a covariate of no interest in your model.\nThere are two things that need to be true to be able to include a confound as a covariate of no interest:\nIt needs to be continuous.\nExactly as it says on the tin - only continuous variables can be included as covariates of no interest in a model.\nThe confound cannot interact with any of your predictors of interest.\nIf there’s a significant interaction between the covariate and any of your predictors, then it can no longer be considered “of no interest”. Let’s revisit the soil type and sunlight example from the top of this page to explain why. In both of the scenarios I showed earlier, there is no interaction between soil type and sunlight - the effects are additive. This means it’s nice and simple to chuck sunlight into the model and let R/Python figure out how much variance to subtract, and what’s left over is for the predictor(s) you really care about.\nHowever, in scenarios such as the one below, this wouldn’t work:\n\n\n\nSunlight cannot be treated as a covariate of no interest\n\n\nThe effect of soil type on growth isn’t now simply being obscured by sunlight - it depends on it. We now need to talk about sunlight in order to be able to describe the effect of soil type. We can no longer discard sunlight as a covariate of no interest.\nThis isn’t necessarily a bad thing, though. This has told us something interesting about plant growth, and indicates that we really should have been thinking about sunlight from the beginning.\nSo, how do you actually include a covariate of no interest in a model?\nGood news! It’s incredibly easy. You’ve probably even done it before without realising.\nIt’s as simple as including the covariate of no interest in the model as if it were any other predictor (though, depending on the exact function you’re using, you may need to ensure that you include it as the first term in the model). And, don’t forget to check for an interaction with other variables by visualising your dataset first.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Confounds and Bias</span>"
    ]
  },
  {
    "objectID": "materials/03-confounds.html#summary",
    "href": "materials/03-confounds.html#summary",
    "title": "5  Confounds and Bias",
    "section": "5.4 Summary",
    "text": "5.4 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nA confounding variable covaries with the predictor(s) and outcome variables\nThere are multiple sources of confounds, including the environment, researcher or technical error, and features of the sample\nMany confounds can be controlled for using techniques such as randomisation, blinding, or matched samples\nSome confounds can be accounted for during analysis, by including them as covariates of no interest in the model",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Confounds and Bias</span>"
    ]
  },
  {
    "objectID": "materials/04-replication.html",
    "href": "materials/04-replication.html",
    "title": "6  Independence and Replication",
    "section": "",
    "text": "6.1 Libraries and functions\nThis section of the course considers the concepts of biological vs technical replication and pseudoreplication. To illustrate them, two example datasets are given. If you want to do the exercises yourself, make sure to check if you have all the required libraries installed.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Independence and Replication</span>"
    ]
  },
  {
    "objectID": "materials/04-replication.html#libraries-and-functions",
    "href": "materials/04-replication.html#libraries-and-functions",
    "title": "6  Independence and Replication",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nRPython\n\n\n\n6.1.1 Libraries\n\n# A collection of R packages designed for data science\nlibrary(tidyverse)\n\n\n\n\n\n6.1.2 Libraries\n\n# A Python data analysis and manipulation tool\nimport pandas as pd\n\n# Simple yet exhaustive stats functions.\nimport pingouin as pg\n\n# Python equivalent of `ggplot2`\nfrom plotnine import *\n\n# Statistical models, conducting tests and statistical data exploration\nimport statsmodels.api as sm\n\n# Convenience interface for specifying models using formula strings and DataFrames\nimport statsmodels.formula.api as smf",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Independence and Replication</span>"
    ]
  },
  {
    "objectID": "materials/04-replication.html#exercise-1---flower-petals",
    "href": "materials/04-replication.html#exercise-1---flower-petals",
    "title": "6  Independence and Replication",
    "section": "6.2 Exercise 1 - Flower petals",
    "text": "6.2 Exercise 1 - Flower petals\nThis dataset contains three variables: shade, which refers to the degree of shading that the plant received while growing; petals, the number of petals recorded on an individual flower of that plant; and plant, the numerical ID assigned to the plant.\n\nRPython\n\n\n\nflowers &lt;- read_csv(\"data/flowers.csv\")\n\n\n\n\n# load the data\nflowers_py = pd.read_csv(\"data/flowers.csv\")\n\n# and have a look\nflowers_py.head()\n\n  shade  petals  plant\n0  none       3      1\n1  none       3      1\n2  none       6      1\n3  none       6      1\n4  none       4      1\n\n\n\n\n\nHaving read in the dataset, we can start by doing some visualisation and analysis. Let’s have a look at how the petal number differs across the shade conditions, and then run a one-way ANOVA to compare the groups statistically.\n\nRPython\n\n\n\n# construct a boxplot, grouped by shade\n\nflowers %&gt;%\n  ggplot(aes(x = shade, y = petals)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nNext, we run a one-way ANOVA:\n\n# create a linear model and run an ANOVA\nlm_flowers &lt;- lm(petals ~ shade, data = flowers)\nanova(lm_flowers)\n\nAnalysis of Variance Table\n\nResponse: petals\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nshade      2  50.70 25.3500   13.51 1.575e-05 ***\nResiduals 57 106.95  1.8763                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# visualise using a boxplot\n(ggplot(flowers_py,\n        aes(x = \"shade\",\n            y = \"petals\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\nNext, we run a one-way ANOVA:\n\n# construct a one-way ANOVA\npg.anova(dv = \"petals\",\n         between = \"shade\",\n         data = flowers_py,\n         detailed = True).round(5)\n\n   Source      SS  DF        MS         F    p-unc     np2\n0   shade   50.70   2  25.35000  13.51052  0.00002  0.3216\n1  Within  106.95  57   1.87632       NaN      NaN     NaN\n\n\n\n\n\nThe plot and one-way ANOVA are both pretty convincing. It looks as if there are most petals on flowers in full sun, and the least petals on flowers in full shade, with partial shade somewhere in the middle.\nHowever, you may have noticed something about this dataset - namely, that multiple measurements of the petals variable have been made per plant. Or, to put it another way, though we have biological replicates by having measured from 12 different plants, our petals measurements appear to be technical replicates.\nThis dataset is a prime example of pseudoreplication.\nLet’s adapt this dataset, by finding the mean petal count per plant.\n\nRPython\n\n\n\nmean_flowers &lt;- flowers %&gt;%\n  group_by(plant, shade) %&gt;%\n  summarise(petals = mean(petals))\n\nmean_flowers\n\n# A tibble: 12 × 3\n# Groups:   plant [12]\n   plant shade   petals\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n 1     1 none       4.4\n 2     2 none       7.2\n 3     3 none       6.6\n 4     4 none       5.8\n 5     5 partial    5.8\n 6     6 partial    5.2\n 7     7 partial    3.4\n 8     8 partial    4.8\n 9     9 full       5  \n10    10 full       4.2\n11    11 full       3.2\n12    12 full       2.6\n\n\n\n\n\nmean_flowers_py = flowers_py.groupby(['plant', 'shade']).mean().reset_index()\n\nmean_flowers_py\n\n    plant    shade  petals\n0       1     none     4.4\n1       2     none     7.2\n2       3     none     6.6\n3       4     none     5.8\n4       5  partial     5.8\n5       6  partial     5.2\n6       7  partial     3.4\n7       8  partial     4.8\n8       9     full     5.0\n9      10     full     4.2\n10     11     full     3.2\n11     12     full     2.6\n\n\n\n\n\nNow, we have a much clearer n = 12. What happens if we re-run our analyses, with these mean values?\n\nRPython\n\n\n\n# construct a new boxplot\nmean_flowers %&gt;%\n  ggplot(aes(x = shade, y = petals)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nRun a new ANOVA:\n\n# ANOVA on the mean petal counts per plant\nlm_mean &lt;- lm(petals ~ shade, data = mean_flowers)\nanova(lm_mean)\n\nAnalysis of Variance Table\n\nResponse: petals\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nshade      2  10.14  5.0700  4.1824 0.05195 .\nResiduals  9  10.91  1.2122                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# visualise using a boxplot\n(ggplot(mean_flowers_py,\n        aes(x = \"shade\",\n            y = \"petals\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\nNext, we run a one-way ANOVA:\n\n# construct a one-way ANOVA\npg.anova(dv = \"petals\",\n         between = \"shade\",\n         data = mean_flowers_py,\n         detailed = True).round(3)\n\n   Source     SS  DF     MS      F  p-unc    np2\n0   shade  10.14   2  5.070  4.182  0.052  0.482\n1  Within  10.91   9  1.212    NaN    NaN    NaN\n\n\n\n\n\nIf anything, the resulting boxplot looks more convincing than it did before. However, we don’t get the same picture with the ANOVA. The p-value is far larger than before, to the point where this analysis is no longer significant. The reason for this is simple - previously, we ran an analysis with a false n = 60, which gave enough power to detect an effect. However, using the true n = 12, we discover that all that statistical power was an illusion or artefact, and with just 12 plants, we can see only the beginning of a trend.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Independence and Replication</span>"
    ]
  },
  {
    "objectID": "materials/04-replication.html#exercise-2---cabbages",
    "href": "materials/04-replication.html#exercise-2---cabbages",
    "title": "6  Independence and Replication",
    "section": "6.3 Exercise 2 - Cabbages",
    "text": "6.3 Exercise 2 - Cabbages\nEach row in the cabbages dataset refers to an individual cabbage, harvested by a farmer who has been trying to find the optimum levels of fertiliser in his six fields. There are four variables: response variable weight, the weight of individual cabbages; N_rate, the rate of nitrogen fertiliser applied to the field in kilograms per metre; fertiliser, a categorical variable describing whether the fertiliser was liquid or granular; and field, the ID of the field that the cabbage was harvested from.\nStart by reading in the dataset. It’s also important that we tell R to treat the N_rate variable as an ordinal variable, or factor, rather than as a continuous numerical variable.\n\nRPython\n\n\n\ncabbages &lt;- read_csv(\"data/cabbages.csv\")\n\n# convert the N_rate column to factor\ncabbages &lt;- cabbages %&gt;%\n  mutate(N_rate = as.factor(N_rate))\n\n\n\n\n# load the data\ncabbages_py = pd.read_csv(\"data/cabbages.csv\")\n\n# convert the N_rate column to factor\ncabbages_py['N_rate'] = cabbages_py['N_rate'].astype('category')\n\n\n\n\nThe farmer is interested in knowing whether nitrogen rate and fertiliser type affects the weight of harvested cabbages in his fields.\nOn the face of it, you may therefore start by fitting a linear model with these two variables as predictors (since they’re both categorical, that’s a two-way ANOVA):\n\nRPython\n\n\n\nlm_cabbage &lt;- lm(weight ~ N_rate * fertiliser, data = cabbages)\nanova(lm_cabbage)\n\nAnalysis of Variance Table\n\nResponse: weight\n                  Df  Sum Sq  Mean Sq F value  Pr(&gt;F)  \nN_rate             2 0.23306 0.116531  4.6861 0.01176 *\nfertiliser         1 0.02891 0.028910  1.1626 0.28402  \nN_rate:fertiliser  2 0.23343 0.116715  4.6935 0.01169 *\nResiduals         84 2.08885 0.024867                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# create a linear model\nmodel = smf.ols(formula = \"weight ~ N_rate * fertiliser\",\n                data = cabbages_py)\n# and get the fitted parameters of the model\nlm_cabbages_py = model.fit()\n\n# look at the model output\nsm.stats.anova_lm(lm_cabbages_py)\n\n                     df    sum_sq   mean_sq         F    PR(&gt;F)\nN_rate              2.0  0.233062  0.116531  4.686123  0.011765\nfertiliser          1.0  0.028910  0.028910  1.162576  0.284018\nN_rate:fertiliser   2.0  0.233429  0.116715  4.693507  0.011687\nResidual           84.0  2.088850  0.024867       NaN       NaN\n\n\n\n\n\nThis indicates that there is a significant interaction between N_rate and fertiliser. To help us visualise the direction of that effect, we can plot the data as follows:\n\nRPython\n\n\n\ncabbages %&gt;%\n  ggplot(aes(x = N_rate, y = weight, fill = fertiliser)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n# visualise using a boxplot\n(ggplot(cabbages_py,\n        aes(x = \"N_rate\",\n            y = \"weight\",\n            fill = \"fertiliser\")) +\n     geom_boxplot())\n\n\n\n\n\n\n\n\n\n\n\nTogether with the ANOVA table, you might be able to make some recommendations to the farmer about the optimum fertiliser programme for his cabbages.\nBut - is this a sensible approach? Do we trust the conclusions?\nTo help you answer that question, let’s visualise the effect of the field variable, and its relationship to other variables, with a plot:\n\nRPython\n\n\n\ncabbages %&gt;%\n  ggplot(aes(x = field, y = weight,\n             colour = fertiliser, size = N_rate)) +\n  geom_point()\n\nWarning: Using size for a discrete variable is not advised.\n\n\n\n\n\n\n\n\n\n\n\n\n# visualise using a boxplot\n(ggplot(cabbages_py,\n        aes(x = \"field\", y = \"weight\",\n            colour = \"fertiliser\",\n            size = \"N_rate\")) +\n     geom_point())\n\n\n\n\n\n\n\n\n\n\n\nThis is rudimentary, but it hopefully helps to illustrate one of two problems with the approach taken here: our different treatments/conditions in the fertiliser and N_rate variables have been applied, wholesale, to entire fields. Which makes sense, practically speaking - it’s hard to see how you would do any differently - but it does mean that there are issues with treating individual cabbages as independent observations, rather than technical replicates.\nHave a think about how you could actually investigate this question, using the dataset presented here. What is our actual value of n? (Or put another way: which are our biological replicates?) What kind of model might you fit instead of the linear model fitted above?",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Independence and Replication</span>"
    ]
  },
  {
    "objectID": "materials/04-replication.html#criteria-for-true-independent-replication",
    "href": "materials/04-replication.html#criteria-for-true-independent-replication",
    "title": "6  Independence and Replication",
    "section": "6.4 Criteria for true independent replication",
    "text": "6.4 Criteria for true independent replication\nConfusing our biological and technical replicates leads to pseudoreplication, as discussed above. So, how do we make sure that we truly do have biological replicates?\nFor a replicate to qualify as a biological, rather than technical replicate, it needs to meet three criteria for independence. These are:\n1) Independent randomisation to different treatment conditions\nThere should be no systematic bias in how biological replicates are allocated to conditions. This means that allocations can’t be made on the basis of sample characteristics. In the first example above, the flowers weren’t randomly assigned to different shade conditions - they were assigned on the basis of which plant they were growing on, meaning that they weren’t independent of one another.\n2) The experimental intervention must be applied independently\nThis is to ensure that any technical error is random. In the second example above, if the farmer incorrectly measures the nitrogren he’s adding to one of his fields, this will affect more than just a single cabbage - it will likely affect a whole group of them, if not the entire field.\n3) Data points/biological replicates must not influence each other\nWhether they are from the same or different conditions, biological replicates shouldn’t have an affect on one another (at least not before you’ve collected the data you need!). This may involve human participants conferring about the study, or in an experiment that involves cell culture, may involve organisms competing with one another for resources and affecting the rate of growth.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Independence and Replication</span>"
    ]
  },
  {
    "objectID": "materials/04-replication.html#summary",
    "href": "materials/04-replication.html#summary",
    "title": "6  Independence and Replication",
    "section": "6.5 Summary",
    "text": "6.5 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nBiological replicates increase n, while technical replicates do not\nThe value of n can have a meaningful impact on the results of significance tests\nPseudoreplication in a sample can lead to a researcher drawing the wrong conclusions",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Independence and Replication</span>"
    ]
  },
  {
    "objectID": "materials/05-power-analysis.html",
    "href": "materials/05-power-analysis.html",
    "title": "7  Statistical Power",
    "section": "",
    "text": "7.1 What is statistical power and power analysis?\nThis section of the course revisits the concept of statistical power, and talks more about the practicalities of pilot experiments.\nPower is defined as the probability of detecting an effect, given that the effect does truly exist in the underlying population of interest. As a general rule, we prefer to see a power of at least 0.8 or 80% in our experiments (and even higher than that is, of course, better).\nPower is mathematically linked with 1) significance/alpha threshold, 2) effect size and 3) sample size. In fact, given three of these, it’s possible to calculate the fourth; this is the underlying principle behind a power analysis. Using the pwr package in R, it’s possible to perform both an a priori power analysis (calculating required sample size, for a given effect size and significance level) and a posteriori power analysis (calculating the overall power of a dataset you’ve already collected).\nI won’t go into any more detail about the nitty-gritty maths behind errors and power here, since there is already an excellent set of course materials that covers it very comprehensively, linked below in the drop-down box:",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "materials/05-power-analysis.html#what-is-statistical-power-and-power-analysis",
    "href": "materials/05-power-analysis.html#what-is-statistical-power-and-power-analysis",
    "title": "7  Statistical Power",
    "section": "",
    "text": "Additional materials\n\n\n\n\n\nTo learn more about performing power analysis for yourself, and a more detailed explanation of effect size, you can view course materials here.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "materials/05-power-analysis.html#why-do-we-care-about-power",
    "href": "materials/05-power-analysis.html#why-do-we-care-about-power",
    "title": "7  Statistical Power",
    "section": "7.2 Why do we care about power?",
    "text": "7.2 Why do we care about power?\nIt is a truth universally acknowledged, that a researcher who’s studying a real, scientific effect must be in want of enough statistical power to detect it.\nOn the face of it, it seems like there’s a very simple solution to make sure that you have enough power: why not just have a massive sample every time? Then, we’re probably going to detect even the smallest effect sizes, and we don’t ever need to worry.\nWell, it’s not quite so simple as that.\nIf you’ve done experiments of your own, you’re probably well aware that they’re expensive and time consuming, and it can be unrealistic to constantly be scaling experiments up (especially without any certainty that the effect of interest is real). In some contexts, such as in animal research, an overly large sample can also be a negative and wasteful thing; or perhaps it simply isn’t feasible, such as if you’re studying a rare disease or condition. A power analysis can help you balance the trade-off between statistical power and feasibility in your research. From the persepective of a funder, or supervisor, it also demonstrates that you’ve thought ahead and planned the experiment carefully, including the resources you’ll need for it.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "materials/05-power-analysis.html#other-factors-to-consider",
    "href": "materials/05-power-analysis.html#other-factors-to-consider",
    "title": "7  Statistical Power",
    "section": "7.3 Other factors to consider",
    "text": "7.3 Other factors to consider\nAside from a small effect size, there are other reasons why your sample size may need to be increased, depending on your planned analysis.\n\nYou’re expecting a high attrition or exclusion rate\nThis is likely to be outside of your control as a researcher, but is important to consider and account for when planning experiments. Attrition and exclusion are both ways in which your sample can be decreased, once you’ve started collecting data.\nExclusion refers to the deliberate choice of an experimenter to remove observations from the sample, perhaps because the participant fails to meet certain criteria, or the quality of the data is not high enough. For instance, in neuroimaging studies, participants’ data may be removed from the study if they moved too much during the functional scan and caused motion artefacts. (Crucially, these exclusions shouldn’t be related to the outcome variable - i.e., there should be no systematic bias in which observations are excluded from the sample, otherwise your dataset is no longer representative, and then we have an entirely new problem besides reduced power!)\nThe term attrition is used to refer to all other reductions in sample size that happen throughout a study, which generally aren’t due to the experimenter. This can include scenarios such as human participants choosing to drop out of a study, a number of plants in the greenhouse dying, or cell cultures failing. Attrition is a particular problem in clinical studies as they recruit from small populations of sometimes vulnerable or unwell participants; further, many clinical studies also have matched samples, which means that one participant dropping out means their match in other experimental group(s) must also be excluded, compounding the difficulty. As a general role, an attrition rate of &gt;20% is considered problematic in a clinical setting.\nThe best way to deal with attrition or exclusion in a sample is to prepare for it in advance, by building in a “buffer” to your sample size. It’s likely that you’ll already know in advance that the type of experiment you’re designing is likely to have a high attrition or exclusion rate; hopefully, you will also have some indication of roughly what that rate might be, based on similar studies conducted by you or colleagues. You should increase your desired final sample size by this amount, so that if the sample is reduced during data collection or analysis, there will still be sufficient power.\n\n\nYou’re planning to separately analyse subsets of the dataset, or make multiple comparisons\nIt’s absolutely acceptable to plan to do either of these two things in your analysis. The trick is to plan for them.\nIf you’re intending to analyse a smaller subset of the data, this means that there needs to be sufficient power within that subset in order to detect the effect(s) of interest. It’s not enough for your overall sample size to be sufficient.\nAlso - analysing multiple subsets of the data will almost always constitute making multiple comparisons; it may be one of the most common ways, in fact, that multiple comparisons are introduced to an analysis pipeline. When making multiple comparisons, it’s typically recommended that you should adjust your significance threshold (or your p-values, which is mathematically equivalent) to reduce the chance of making a type I error. Increasing your significance threshold necessarily will reduce power, however, because of MathsTM. So, when making lots of multiple comparisons, you need a larger sample to boost power.\n\n\nYou know (or suspect) that there will be lots of parameters in your model\nThis includes, broadly, two sets of scenarios: 1) you have lots of predictors of interest, and/or 2) you have lots of uncontrolled variables that can affect your outcome unpredictably, that you plan to include as covariates of no interest. The overall impact on power is the same in both cases. Regardless of whether a variable is a predictor of interest, its inclusion in the model will still decrease the degrees of freedom, as more parameters need to be estimated, and increase the model’s complexity.\nWith any luck, you’ll be able to perform model comparison and simplify your model somewhat after building it (as a reminder: we are always looking for the simplest model that does a good job of explaining the variance in the dataset, in the goodness-of-fit vs complexity trade-off). But it’s best to make sure that you account for all possible parameters when performing a priori power analyses.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "materials/05-power-analysis.html#summary",
    "href": "materials/05-power-analysis.html#summary",
    "title": "7  Statistical Power",
    "section": "7.4 Summary",
    "text": "7.4 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nError, significance, effect size and power are all related to one another, such that you can calculate desired sample size a priori\nMultiple factors can decrease power, including making corrections for multiple comparisons and including a large number of parameters in your model\nIt’s also important to be aware of decreases in sample size from attrition and exclusion, which will reduce power",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical Power</span>"
    ]
  },
  {
    "objectID": "materials/06-piloting.html",
    "href": "materials/06-piloting.html",
    "title": "8  Pilot Studies",
    "section": "",
    "text": "8.1 Purpose(s) of a pilot study\nIf you’re planning an experiment, but still have some important open questions about the protocol, desired sample size or planned analysis, then you may wish to run a pilot study in advance of conducting the experiment. Its exact nature can differ depending on what you’re trying to get out of the it, but in most cases the pilot study w be a small-scale, preliminary study that is conducted ahead of the main experiment, in order to inform the final experimental design in some way. Essentially, it serves as a “trial run” of the final experiment.\nThere are different reasons to run a pilot study, and they might depend on what field of research you’re in. These reasons can include (but are not necessarily limited to!):",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pilot Studies</span>"
    ]
  },
  {
    "objectID": "materials/06-piloting.html#purposes-of-a-pilot-study",
    "href": "materials/06-piloting.html#purposes-of-a-pilot-study",
    "title": "8  Pilot Studies",
    "section": "",
    "text": "Testing and refining your protocol or methodology\nThis may include things like calibrating equipment or testing out a new behavioural task or set of stimuli. It’s also likely to involve spotting errors or problems that you would otherwise have needed to deal with during the main experiment - perhaps your instructions to participants or fellow researchers are unclear and need refining, or perhaps the lab stocks of the media you need are too low.\nNote that this does not mean that you should go into your pilot study without a protocol or plan. You should have already worked out as many of the details as possible, such as how you plan to operationalise your variables, whether there are any known factors to control, and an idea of the type of analysis you might run. The more detailed your plan is before you pilot, the easier it will be to make refinements!\n\n\nTraining researchers\nThis ties in closely with the point above. The experiment may be the first time that you or other researchers have used certain protocols or equipment before, and research involves lots of skills that take practice - whether that involves interacting with patients, learning how to safely use potentially dangerous equipment, or complicated/precise bench work.\n\n\nDemonstrating feasibility & exploring possible results\nSometimes, a pilot can serve as a low-risk way to check whether something is likely to work at all. For instance: is it possible to get high enough resolution images of this phenomenon? Is the rare patient population willing to take part in this sort of study? Will this plant species grow at all in our greenhouses? Are we likely to have a manageable exclusion or attrition rate?\nIf you’re lucky, your pilot study may also give you some idea as to the likely results that a larger study will give you; more on this in the point below. But it’s worth emphasising at this stage - this does not mean you should use a pilot study to perform hypothesis testing of your overall research question! (For instance, if you get a significant result in your pilot sample, this doesn’t mean that you don’t still need to perform the main experiment.)\n\n\nCalculating an effect size for power analysis\nGiven a pilot dataset, it’s possible to calculate the effect size (using either the difference between group means for a t-test, or from the R2 value for linear models) for use in a power analysis - even if/when the pilot sample doesn’t yield any significant results.\nIt’s worth noting, however, that doing this can be a bit contentious. Some people will tell you that estimating an effect size from a pilot study is a bad idea, because smaller pilot samples are very noisy, and our confidence interval for calculated effect size from small samples is therefore very wide. Such people will tell you that the better approach is to determmine the smallest scientifically meaningful effect size that you’re interested in detecting, and use that for your power analyses instead. This raises the question, of course, of what “scientifically meaningful” means; this is often easier to determine in clinical and drug studies, than it is in more basic scientific research. But you can use standard “small”, “medium” and “large” effect sizes to guide you on this, or alternatively, you can look at existing full-scale studies (e.g., existing publications, unpublished data from your group or collaborators) to give you an idea of what sort of effect sizes might be expected for the type of research you plan to conduct. (These course materials cover effect size in a little more detail, if you’re curious.)\nAs a compromise between these approaches, you might use the estimated effect size from a pilot study to supplement an estimate from a previous full-scale study, to determine whether your planned experiment is likely to yield similar effect sizes to comparable research.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pilot Studies</span>"
    ]
  },
  {
    "objectID": "materials/06-piloting.html#pilot-sample",
    "href": "materials/06-piloting.html#pilot-sample",
    "title": "8  Pilot Studies",
    "section": "8.2 Pilot sample",
    "text": "8.2 Pilot sample\nDeciding how large your pilot sample should be is often a difficult exercise - you’re trying to make a difficult trade-off between gathering information, versus not launching into a full experiment.\nIf you plug the question “how big should my pilot sample be?” into Google, you may see a couple of common rules-of-thumb: “10-20% of the final desired sample size”, “12 observations in each group”, and so on. These aren’t bad starting points, but they do assume that you’re doing a certain type of research (often, these are written for clinical researchers). There are situations where the size of your pilot sample might be guided by other factors.\nIf you’re hoping to use your pilot to get the best possible effect size estimate, then you will likely want to push your pilot to be as large as possible, to get the narrowest confidence interval for the effect size that you possibly can.\nIf, conversely, you’re using the pilot simply to test feasibility (e.g., can we image this phenomenon? Will this reaction occur under these conditions?) then you may go for the minimum sample needed to confirm feasibility, and stop once you’ve achieved that.\nIf your intention is to make iterative changes and improvements on your protocol, then you should consider very carefully whether you truly have a single pilot study, or whether it would be better described as a series of pilots. In any case, you should ensure that the final version of the experiment that you pilot, has a sufficient sample size in itself for you to be happy that the changes and revisions you’ve made have had the effect you wanted.\n\n\n\n\n\n\nAn important note about keeping your pilot sample separate\n\n\n\n\n\nIt’s quite common to see researchers including their pilot sample in the main dataset, once they conduct the larger experiment. This is an easy temptation to fall for, but it’s often best to avoid doing so.\nFirstly, your pilot study will almost certainly be imperfect in some way. If you have been refining or updating your protocol or training researchers as part of your pilot, then the data quality may be lower in the pilot dataset. Worse still, differences between the pilot and main protocol might introduce a confounding variable, if those differences have a meaningful impact on your response variable.\nFrom a purely statistical point of view, there is also the possibility of increasing error by folding your pilot sample into your main sample, particularly if you hadn’t planned to do so in advance. Analysing your pilot sample, and then re-analysing these data as part of the experimental sample, also constitutes making multiple comparisons, which can increase your chance of a type I error (false positive).\nOf course, as with a lot of what these course materials talk about, there are scenarios where these concerns might be outweighed. For instance, if you are working with a very rare population, it might not be feasible to discard any of the data you’ve collected; maintaining as large a sample as possible might be your priority. In these cases, you could choose to make adjustments in your analysis (e.g., including covariates) to account for any differences in protocol or equipment.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pilot Studies</span>"
    ]
  },
  {
    "objectID": "materials/06-piloting.html#summary",
    "href": "materials/06-piloting.html#summary",
    "title": "8  Pilot Studies",
    "section": "8.3 Summary",
    "text": "8.3 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nA pilot study can be considered a “trial run” for your experiment\nThe pilot can be used for testing/refining your protocol, training researchers, demonstrating feasibility and sometimes for estimating effect sizes\nThe required sample size for a pilot will depend on the purpose and field of research",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Pilot Studies</span>"
    ]
  },
  {
    "objectID": "materials/07-case-study.html",
    "href": "materials/07-case-study.html",
    "title": "9  Case Studies",
    "section": "",
    "text": "9.1 Case Study 1 - Plant growth & light colour\nThis final section of the course provides an opportunity to put the content learned so far into practice.\nChoose a case study, and design the experiment you would conduct to answer the research question.\nConsider:\nIt is established that both the intensity and colour of light (balance of red, green and blue light) can affect plant growth. However, you’re interested in knowing how this relationship holds specifically in common varieties of British ferns.\nTo investigate this, you have access to the following resources, plus other reasonable resources:\nSome additional considerations:",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "materials/07-case-study.html#case-study-1---plant-growth-light-colour",
    "href": "materials/07-case-study.html#case-study-1---plant-growth-light-colour",
    "title": "9  Case Studies",
    "section": "",
    "text": "Seedlings (unlimited supply) from 3 different common species of fern.\nThree greenhouses to plant seedlings, with a capacity of 150 plants each.\nGrow lights (unlimited supply) that allow you to vary the ratio of green, red and blue light as desired. They come in one of three intensities: 500, 1000 or 2500 lumens per square foot.\n\n\n\nNot all plants will survive; around 5% of seedlings in a greenhouse are expected to die, at random.\nHow will you operationalise growth? Plants typically grow differently depending on their location in a greenhouse – for example, plants near the centre will grow fewer branches/shoots, as they grow upward rather than outward.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "materials/07-case-study.html#case-study-2---teaching-experience-in-university-lecturers",
    "href": "materials/07-case-study.html#case-study-2---teaching-experience-in-university-lecturers",
    "title": "9  Case Studies",
    "section": "9.2 Case Study 2 - Teaching experience in university lecturers",
    "text": "9.2 Case Study 2 - Teaching experience in university lecturers\nA university is interested in testing whether a lecturer’s number of years of teaching experience can predict the quality of their teaching.\nTo investigate this, you have access to the following, plus other reasonable information or resources:\n\nAnonymised student grades from the last 5 years for 20 lecturers at the university. Eleven of these are from humanities, and nine from life sciences. Each year, each lecturer teaches between 50-100 students.\nThe ability to distribute surveys to current students for all 20 lecturers.\nDemographic information for the lecturers, such as gender, DOB, the year they began teaching and the number of years spent in research.\n\nSome considerations:\n\nWhat is n?\nAre there any covariates of no interest?\nHow do you operationalise “quality of teaching”?",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "materials/07-case-study.html#case-study-3---caffeine-and-memory-in-rats",
    "href": "materials/07-case-study.html#case-study-3---caffeine-and-memory-in-rats",
    "title": "9  Case Studies",
    "section": "9.3 Case Study 3 - Caffeine and memory in rats",
    "text": "9.3 Case Study 3 - Caffeine and memory in rats\nAn animal psychologist is interested in determining the effects of caffeine consumption on rats’ ability to complete memory tasks.\nTo investigate this, you have access to the following, plus other reasonable resources:\n\nUp to 50 naïve rats that have never been trained on memory tasks before.\nThe ability to give the rats controlled doses of caffeine, and/or a placebo.\nThree common tasks for memory in rats: the Morris water maze, for spatial memory; the radial arm maze, for working memory; and an object recognition task.\n\nWhen planning, consider:\n\nHow will you gauge baseline memory ability in the rats?\nWill you use a repeated or independent measures design?\nShould counterbalancing be used? What about blinding?\nHow can power analysis help you to adhere to the “Reduction” principle in animal research (i.e., minimising the number of animals used)?",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "materials/08-simulating-datasets.html",
    "href": "materials/08-simulating-datasets.html",
    "title": "10  Simulating Datasets",
    "section": "",
    "text": "10.1 Libraries and functions\nThis section of the course teaches you how to simulate simple biological datasets, including categorical predictors, and continuous predictors and interaction effects. The materials then briefly introduce how this method can be used to help design experiments.\nCurrently, this chapter is written only in R. Python code will be added at a later date.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulating Datasets</span>"
    ]
  },
  {
    "objectID": "materials/08-simulating-datasets.html#libraries-and-functions",
    "href": "materials/08-simulating-datasets.html#libraries-and-functions",
    "title": "10  Simulating Datasets",
    "section": "",
    "text": "Click to expand\n\n\n\n\n\n\nR\n\n\n\n10.1.1 Libraries\n\nlibrary(tidyverse)\nlibrary(rstatix)\n\n# These packages will be used for evaluating the models we fit to our simulated data\nlibrary(performance)\nlibrary(ggResidpanel)\n\n# This package is optional/will only be used for later sections in this chapter\nlibrary(MASS)",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulating Datasets</span>"
    ]
  },
  {
    "objectID": "materials/08-simulating-datasets.html#drawing-samples-from-distributions",
    "href": "materials/08-simulating-datasets.html#drawing-samples-from-distributions",
    "title": "10  Simulating Datasets",
    "section": "10.2 Drawing samples from distributions",
    "text": "10.2 Drawing samples from distributions\nThe first thing we need to get comfortable with is random sampling, i.e., drawing a number of datapoints from an underlying distribution with known parameters.\n(Remember that distributions have parameters that describe their characteristics/shape; when we calculate descriptive statistics in datasets, we are estimating those parameters.)\n\nR\n\n\n\nrnorm(n = 100, mean = 0, sd = 1)\n\n  [1]  0.071374340 -0.326355950 -0.316272060  0.714762914  1.231117797\n  [6]  0.177889318  1.890878877 -0.444222309  0.999440811  1.325929186\n [11] -1.594955236 -2.763780102  1.068719311 -2.013290802 -0.047840694\n [16]  0.682022757 -1.562907002  0.889700764 -0.211819600  0.225171724\n [21]  0.707758495  0.937594656 -0.259375196  1.541067189 -0.202421575\n [26] -0.189609183  0.001489118  0.104905602  0.564455173 -0.056826022\n [31]  0.298151885  0.659031358  0.744258318 -1.153980494 -0.159347585\n [36]  0.109633598 -0.399605383 -0.247957402  0.573187133 -0.876074007\n [41] -1.052777472 -0.425085299  0.154539092  0.537239238 -0.080133397\n [46]  0.521204511 -0.564676474 -0.925795414 -0.969224973 -0.985596470\n [51] -2.138170742  0.513790248 -1.434767684 -1.265311403  0.332237661\n [56] -0.414106781  1.861226577 -0.138663930  1.153357684  0.698008599\n [61]  1.264656392 -0.528106386  1.424556451  0.514379429 -0.054746768\n [66] -0.565047287 -1.015994414 -1.145573974  1.673531210  0.676893653\n [71] -0.531797383  1.195813627  1.085429174 -0.531826834  0.739816678\n [76] -0.368723878  0.205830349 -0.735577755  0.271823276  0.751570361\n [81]  0.559010273  0.081037327  1.395208492 -0.158576681  0.042000520\n [86]  0.504998600 -1.848816264  0.860488034 -0.398646201 -0.387246596\n [91] -1.524281990 -0.554137954 -0.078332202  1.196901350 -1.630853334\n [96] -0.314530194  1.096844693  0.575956885 -0.777824015 -0.102299248\n\n\nThis is the rnorm function. It takes three arguments. The first is the number of datapoints (n) that you’d like to draw. The second and third arguments are the two important parameters that describe the shape of the underlying distribution: the mean and the standard deviation.\nNow, rather than just getting a string of numbers, let’s visualise the dataset we’ve sampled.\nWe’ll use the base R hist function for this to keep things simple:\n\nrnorm(100, 0, 1) %&gt;%\n  hist()\n\n\n\n\n\n\n\n\n\n\n\nRerun the last line of code multiple times. What happens?\n\n\n\n\n\n\nSetting a seed\n\n\n\n\n\nYou may have noticed that when we repeat the above code over and over, we are sampling a different random subset of data points each time.\nSometimes, it’s useful for us to be able to sample the exact set of data points more than once, however.\nTo achieve this, we can use the set.seed function.\nRun the following code several times in a row, and you’ll see the difference:\n\nR\n\n\n\nset.seed(20)\n\nrnorm(100, 0, 1) %&gt;%\n  hist()\n\n\n\n\nYou can choose any number you like for the seed. All that matters is that you return to that same seed number, if you want to recreate that dataset.\n\n\n\n\n10.2.1 Revisiting Shapiro-Wilk\nTo help link this sampling procedure back to some statistics that you might be familiar with, let’s use it to explore the Shapiro-Wilk test. It’s a null hypothesis significance test, used to help us decide whether a sample has been drawn from a normally-distributed underlying population.\n\nR\n\n\n\nset.seed(200)\n\nrnorm(100, 0, 1) %&gt;%\n  hist()\n\n\n\n\n\n\n\nrnorm(100, 0, 1) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.9907, p-value = 0.7208\n\n\n\n\n\nAs expected, these data generate an insignificant Shapiro-Wilk test: we retain the null hypothesis, and infer that the data have come from a normal distribution. We know this is true, so we can confirm we have a true negative result.\nHowever, let’s look at a different seed. Note that we’re keeping everything else identical about the code, including the underlying parameters/nature of the distribution.\n\nR\n\n\n\nset.seed(20)\n\nrnorm(100, 0, 1) %&gt;%\n  hist()\n\n\n\n\n\n\n\nrnorm(100, 0, 1) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.96715, p-value = 0.01343\n\n\n\n\n\nThis test gives us a significant result - suggesting non-normality - even though we know full well that the underlying distribution is normal, because we made it so!\nWhat’s happened? Well, this is a classic case of a false positive error: we reject the null hypothesis that the population is normal, even though it was actually true. We just got unlucky that the sample from seed 20 is behaving a bit unusually, so we make the wrong inference.\nLet’s compare this to situations where we know that the null hypothesis is false, i.e., the underlying population isn’t normal.\nThis also gives us the chance to introduce the runif function, which works similarly to rnorm. It samples from a uniform distribution, with a specific minimum and maximum that we set as the parameters/arguments for the function:\n\nR\n\n\n\nset.seed(20)\n\nrunif(100, min = 0, max = 1) %&gt;%\n  hist()\n\n\n\n\n\n\n\nrunif(100, 0, 1) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.94018, p-value = 0.0001976\n\n\n\n\n\nAs expected, we are told that these data are very unlikely to occur if the null hypothesis is true. This is a true positive result: we correctly infer that the underlying distribution is not normal.\nBut, just as we showed that the Shapiro-Wilk test can make a false positive error, it can also make a false negative error (missing a real result).\nTo force this to occur, we’re going to make the test deliberately under-powered, by drastically reducing the sample size:\n\nR\n\n\n\nset.seed(20)\n\nrunif(10, min = 0, max = 1) %&gt;%\n  hist()\n\n\n\n\n\n\n\nrunif(10, 0, 1) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.90646, p-value = 0.2576\n\n\n\n\n\nAnd there we have it - a false negative error! Even though the underlying population isn’t normal, the “signal” (the non-normality) isn’t strong enough to overcome the noisiness of a small dataset.\nAs an exercise, try creating normal QQ plots for each of these datasets, by using the base R qqnorm function, like so.\n\nR\n\n\n\nset.seed(20)\n\nrnorm(100, 0, 1) %&gt;%\n  qqnorm()\n\n\n\n\n\n\n\n\n\n\n\nCompare these QQ plots with the Shapiro-Wilk test results. Do you think you would make different decisions using the two methods?",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulating Datasets</span>"
    ]
  },
  {
    "objectID": "materials/08-simulating-datasets.html#simulating-a-continuous-predictor",
    "href": "materials/08-simulating-datasets.html#simulating-a-continuous-predictor",
    "title": "10  Simulating Datasets",
    "section": "10.3 Simulating a continuous predictor",
    "text": "10.3 Simulating a continuous predictor\nNow that the QQ plot propaganda is out of the way, let’s extend the simulations by making our dataset two-dimensional (rather than a single dimension/list of data points).\nSpecifically, let’s simulate a situation where one continuous variable predicts another. This is the situation that occurs in a simple linear regression.\nTo simulate this situation, and all the others below it in this chapter, we will start by setting a seed. We’ll also set a value of n straight away.\n\nR\n\n\n\nset.seed(20)\n\n# sample size\nn &lt;- 60\n\n\n\n\nNow, we’re going to generate our predictor variable. There’s no noise or uncertainty in our predictor (remember that residuals are always in the y direction, not the x direction), so we can just produce the values by sampling from a distribution of our choice.\nDepending on the nature of the variable we’re simulating, we might think that various different distributions are more appropriate or representative.\nFor the example here, we’re going to simulate a dataset about golden toads, an extinct species of amphibians.\nHere’s what they looked like - pretty fancy guys, no?\n\n\n\nThere’s space for you both, Jack!\n\n\nOur response variable will be the clutch size (number of eggs).\nOne of the things that can cause variation in clutch size is the size of the toad herself, so we’ll use that as our continuous predictor, and we’ll sample it from a normal distribution.\nGoogle tells us that the average female golden toad was somewhere in the region of 42-56mm long, so we’ll use that as a sensible basis for our normal distribution for our predictor variable length.\n\nR\n\n\n\nlength &lt;- rnorm(n, 48, 3)\n\n\n\n\nNow, we need to simulate our response variable, clutchsize.\nWe’re going to do this by setting up the linear model. We’ll specify a y-intercept for clutchsize, plus a gradient that captures how much clutchsize changes as length changes.\n\nR\n\n\n\nb0 &lt;- 175\nb1 &lt;- 2\n\nsdi &lt;- 20\n\n\n\n\nWe’ve also added an sdi parameter. This captures the standard deviation around the model predictions that is due to other factors we’re not measuring. In other words, this will determine the size of our residuals.\nNow, we can simulate our set of predicted values for clutchsize.\n\nR\n\n\n\navg_clutch &lt;- b0 + b1*length\n\n\n\n\nYou’ll notice we’ve just written out the equation of our model.\n\nR\n\n\n\ntibble(length, avg_clutch) %&gt;%\n  ggplot(aes(x = length, y = avg_clutch)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nWhen we visualise length and avg_clutch together, you see they perfectly form a straight line. That’s because avg_clutch doesn’t contain the residuals - that comes next.\nThe final step is to simulate the actual values of clutch size.\nWe’ll use rnorm function again, and we put avg_clutch in as our mean. This is because the set of actual clutch size values should be normally distributed around our set of predictions - this is what we mean when we say that the residuals should be normally distributed!\n\nR\n\n\n\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\ngoldentoad &lt;- tibble(clutchsize, length)\n\n\n\n\nThen we use the tibble function to combine our response and predictor variables together into a dataset that we can explore.\n\n10.3.1 Checking our dataset\nLet’s make sure our dataset is behaving the way we intended.\nFirst, we’ll visualise it:\n\nR\n\n\n\nggplot(goldentoad, aes(x = length, y = clutchsize)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nAnd then, we’ll construct a linear model - and check that our beta coefficients have been replicated to a sensible level of precision!\n\nR\n\n\n\nlm_golden &lt;- lm(clutchsize ~ length, goldentoad)\n\nsummary(lm_golden)\n\n\nCall:\nlm(formula = clutchsize ~ length, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.718 -10.973   1.094  11.941  41.690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 267.1395    38.4922   6.940  3.7e-09 ***\nlength        0.1065     0.8038   0.132    0.895    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.76 on 58 degrees of freedom\nMultiple R-squared:  0.0003025, Adjusted R-squared:  -0.01693 \nF-statistic: 0.01755 on 1 and 58 DF,  p-value: 0.8951\n\n\n\n\n\nNot bad at all. The linear model has managed to extract beta coefficients very close to the original b0 and b1 that we set.\nIf you’re looking to explore and understand this further, try exploring the following things in your simulation, and see how they affect the p-value and the precision of the beta estimates:\n\nVarying the sample size\nVarying the sdi\nVarying the b1 parameter",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulating Datasets</span>"
    ]
  },
  {
    "objectID": "materials/08-simulating-datasets.html#simulating-a-categorical-predictor",
    "href": "materials/08-simulating-datasets.html#simulating-a-categorical-predictor",
    "title": "10  Simulating Datasets",
    "section": "10.4 Simulating a categorical predictor",
    "text": "10.4 Simulating a categorical predictor\nCategorical predictors are a tiny bit more complex to simulate, as the beta coefficients switch from being constants (gradients) to vectors (representing multiple means).\nLet’s imagine that golden toads living in different ponds produce slightly different clutch sizes, and simulate some sensible data on that basis.\nBefore we do anything else, let’s clear our global environment so that nothing from our previous simulation has an unexpected impact on our new one:\n\nR\n\n\n\nrm(list=ls())\n\n\n\n\nThen, we’ll set up the parameters and predictor variables:\n\nR\n\n\n\nset.seed(20)\n\nn &lt;- 60\nb0 &lt;- 175\nb1 &lt;- 2\nb2 &lt;- c(0, 30, -10)\n\nsdi &lt;- 20\n\nlength &lt;- rnorm(n, 48, 3)\npond &lt;- rep(c(\"A\", \"B\", \"C\"), each = n/3)\n\n\n\n\nWe’ve set up a beta coefficient for our categorical predictor, which consists of three categories. The ponds have imaginatively been named A, B and C.\nNote the use of the rep and c functions to generate our values for the categorical predictor - these functions are very much your friend in simulating datasets!\nOnce again, we simulate a set of predicted values using the model equation. We use the equation from above, but add our extra predictor/term.\n\nR\n\n\n\navg_clutch &lt;- b0 + b1*length + model.matrix(~0+pond) %*% b2\n\nIncluding a categorical predictor is a bit more complex. We use model.matrix(~0+pond) %*% b2 instead of simply multiplying our variable by a constant.\nThe model.matrix function produces a table of 0s and 1s - a matrix that represents the design of our experiment. Our b2 is also technically a matrix. Then, %*% syntax is the operator in R for matrix multiplication, to multiply these two things together.\nYou don’t really need to understand matrix multiplication to get used to this method. We’ll use this syntax a few more times in this chapter, so you’ll learn to recognise and repeat the pattern - that’s plenty!\n\n\n\nFinally, as before, we now sample our actual values of clutchsize from a normal distribution with avg_clutch as the mean and with a standard deviation of sdi.\n\nR\n\n\n\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\ngoldentoad &lt;- tibble(clutchsize, length, pond)\n\n\n\n\n\n10.4.1 Check the dataset\nOnce again, we’ll visualise and model these data, to check that they look as we suspected they would.\n\nR\n\n\n\nlm_golden2 &lt;- lm(clutchsize ~ length + pond, goldentoad)\n\nsummary(lm_golden2)\n\n\nCall:\nlm(formula = clutchsize ~ length + pond, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.202 -11.751  -0.899  14.254  37.039 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 262.8111    38.6877   6.793  7.6e-09 ***\nlength        0.1016     0.8108   0.125    0.901    \npondB        39.2084     5.9116   6.632  1.4e-08 ***\npondC        -5.5279     5.9691  -0.926    0.358    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.69 on 56 degrees of freedom\nMultiple R-squared:  0.5481,    Adjusted R-squared:  0.5239 \nF-statistic: 22.64 on 3 and 56 DF,  p-value: 9.96e-10\n\nggplot(goldentoad, aes(x = length, y = clutchsize, colour = pond)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nHas our model recreated “reality” very well? Would we draw the right conclusions from it?\nOnce again: explore what happens if you change different parameters in your model.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulating Datasets</span>"
    ]
  },
  {
    "objectID": "materials/08-simulating-datasets.html#simulating-interactions",
    "href": "materials/08-simulating-datasets.html#simulating-interactions",
    "title": "10  Simulating Datasets",
    "section": "10.5 Simulating interactions",
    "text": "10.5 Simulating interactions\nNow, let’s simulate an interaction effect length:pond.\nSince at least one of the variables in our interaction is a categorical predictor, requiring a vector beta coefficient and the use of the model.matrix syntax, the interaction will be the same.\nThink of it this way: our model with an interaction term will consist of three lines of best fit, each with a different intercept and gradient.\nThe difference in intercepts is captured by b2, and then the difference in gradients is captured by b3 that we set now:\n\nR\n\n\n\nrm(list=ls())\n\nset.seed(20)\n\nn &lt;- 60\nb0 &lt;- 175\nb1 &lt;- 2\nb2 &lt;- c(0, 30, -10)\nb3 &lt;- c(0, 0.5, -0.2)\n\nsdi &lt;- 20\n\nlength &lt;- rnorm(n, 48, 3)\npond &lt;- rep(c(\"A\", \"B\", \"C\"), each = n/3)\n\n\n\n\nAnd then we continue exactly as we did before. We don’t need to set up a new predictor, since we’re just using the two we were before.\n\nR\n\n\n\navg_clutch &lt;- b0 + b1*length + model.matrix(~0+pond) %*% b2 + model.matrix(~0+length:pond) %*% b3\n\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\ngoldentoad &lt;- tibble(clutchsize, length, pond)\n\n\n\n\n\n10.5.1 Checking the dataset\n\nR\n\n\n\nggplot(goldentoad, aes(x = length, y = clutchsize, colour = pond)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nlm_golden3 &lt;- lm(clutchsize ~ length*pond, goldentoad)\n\nsummary(lm_golden3)\n\n\nCall:\nlm(formula = clutchsize ~ length * pond, data = goldentoad)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.409 -11.136  -1.377  12.863  37.095 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  271.19106   64.37630   4.213 9.64e-05 ***\nlength        -0.07503    1.35414  -0.055    0.956    \npondB         11.00129   89.05626   0.124    0.902    \npondC          8.73498  106.35392   0.082    0.935    \nlength:pondB   1.09421    1.87219   0.584    0.561    \nlength:pondC  -0.49062    2.20866  -0.222    0.825    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19 on 54 degrees of freedom\nMultiple R-squared:  0.7793,    Adjusted R-squared:  0.7588 \nF-statistic: 38.13 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n10.5.2 Exercise 1 - Including confounds\n\nLevel: \nOften in research there are additional factors that vary during our experiment, which have an impact on our response variable and yet we are not actually interested in.\nThese are often referred to as confounding variables, and one of the methods of dealing with them (if you can’t control for them in your initial data collection) is to include them as covariates of no interest in your model.\nModelling confounds, or covariates of no interest, is as simple as just including them as predictors and then ignoring them in your interpretation (with the caveat that if the covariate has a meaningful interaction with a predictor of interest, you will need to mention it in the interpretation after all!)\nThis means that simulating covariates of no interest is just as easy as simulating any other predictor variable.\n\nAdd a froggy confound!\nTo practise what you’ve seen above:\n\nAdd one or more covariates to your simulation. Start by including it as a main effect, with no interactions. You can choose whether you’d like them to be categorical or continuous - it makes no difference!\nThen, simulate a version of the dataset where your covariate of choice has an interaction with one of the predictors we do care about.\nFit a model to this new version of the dataset. Deliberately exclude the interaction term from that model.\n\nNote how this impairs your ability to access the true relationship between your predictors and your response.\nThis is why it’s so important to consider interactions!",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulating Datasets</span>"
    ]
  },
  {
    "objectID": "materials/08-simulating-datasets.html#power-analysis",
    "href": "materials/08-simulating-datasets.html#power-analysis",
    "title": "10  Simulating Datasets",
    "section": "10.6 Power analysis",
    "text": "10.6 Power analysis\nThus far, we’ve used simulation as a way of better understanding how linear models work. This has a few useful applications in real research:\n\nIt lets you “imagine” your data more clearly…\n… which, in turn, may help you identify flaws in your experimental design\nYou can test out your analysis pipelines (e.g., scripts) on simulated data, for debugging purposes\n\nAnother, very meaningful application of data simulation is constructing your very own power analysis. The idea is that you can simulate many datasets from the same model (instead of just one, as we’ve been doing), fit the model to each of them, and then look across the entire set to see how it performs across the board.\nLet’s try that.\nWe’ll use the simple linear regression example from above, to keep things as transparent as possible.\nWe’ll extract the overall F-statistic and p-value, and also the R2 value, just as an example. You could use this code as a template to also extract the beta coefficients, or individual p-values for multiple predictors (i.e., the output of an anova function).\n\nR\n\n\n\nrm(list=ls())\n\nset.seed(20)\n\n# As always, set the parameters that won't change\nn &lt;- 60\nb0 &lt;- 175\nb1 &lt;- 2\nsdi &lt;- 20\n\nNow, we use a for loop to simulate 1000 datasets, and save their p-values in a list object.\n\n# First, we have to initialise the matrix that we plan to store our info in\ntoad_sim_results &lt;- data.frame(matrix(ncol=3,nrow=1000, dimnames=list(NULL, c(\"f\", \"p\", \"rsquared\"))))\n\nfor(i in 1:1000) {\n\n  length &lt;- rnorm(n, 48, 3)\n  avg_clutch &lt;- b0 + b1*length\n  clutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\n  goldentoad &lt;- tibble(length, clutchsize)\n\n  lm_toad &lt;- lm(clutchsize ~ length)\n  \n  f &lt;- summary(lm_toad)$fstatistic\n  toad_sim_results$f[i] &lt;- f[1]\n  toad_sim_results$p[i] &lt;- pf(f[1],f[2],f[3],lower.tail=F)\n  toad_sim_results$rsquared[i] &lt;- summary(lm_toad)$r.squared\n\n}\n\n\n\n\nNow, we can look at the overall distribution of these different values.\nLet’s start by looking at the test statistics.\n\nR\n\n\n\nplot(toad_sim_results$f)\n\n\n\n\n\n\n\nhist(toad_sim_results$f)\n\n\n\n\n\n\n\n\n\n\n\nRemember: each of these F-statistics is acting as a “summary” for a single dataset, of pertinent information about those data (in this case, the signal-to-noise ratio). Since each of our samples is unique, although drawn from the same underlying distribution, we also expect them to have different F-statistics.\n(Reassuringly, our 1000 F-statistics seem to follow an F-distribution - this makes sense!)\nThe interesting part for a power analysis, however, is figuring out what proportion of these F-statistics have associated p-values under our significance threshold.\nLet’s assume that we’re using a significance threshold of 0.05.\n\nR\n\n\n\nmean(toad_sim_results$p &lt; 0.05, na.rm = TRUE)\n\n[1] 0.615\n\n\n\n\n\n64% of our results are significant. Specifically, 64% of our tests give significant results, in a situation where the null hypothesis is false (i.e., there is a real result). These represent our true positives.\nThe “true positive” rate is our statistical power. With a given effect size and sample size - which we set up by choosing n, sdi and the beta coefficients - we find that this model has 64% power.\nWe have performed an a posteriori power analysis.\nLet’s compare this to a traditional power analysis, using a couple of specific datasets simulated under different seeds, but using the same parameters.\n\nR\n\n\n\nrm(list=ls())\n\nset.seed(21)\n\nn &lt;- 60\nb0 &lt;- 175\nb1 &lt;- 2\nsdi &lt;- 20\n\nlength &lt;- rnorm(n, 48, 3)\navg_clutch &lt;- b0 + b1*length\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\ngoldentoad &lt;- tibble(length, clutchsize)\n\nlm_toad &lt;- lm(clutchsize ~ length)\n\nu &lt;- summary(lm_toad)$f[2]\nv &lt;- summary(lm_toad)$f[3]\nf2 &lt;- summary(lm_toad)$r.squared/(1-summary(lm_toad)$r.squared)\n  \npwr.f2.test(u, v, f2, sig.level=0.05)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 58\n             f2 = 0.3092493\n      sig.level = 0.05\n          power = 0.9885543\n\n\n\nrm(list=ls())\n\nset.seed(23)\n\nn &lt;- 60\nb0 &lt;- 175\nb1 &lt;- 2\nsdi &lt;- 20\n\nlength &lt;- rnorm(n, 48, 3)\navg_clutch &lt;- b0 + b1*length\nclutchsize &lt;- rnorm(n, avg_clutch, sdi)\n\ngoldentoad &lt;- tibble(length, clutchsize)\n\nlm_toad &lt;- lm(clutchsize ~ length)\n\nu &lt;- summary(lm_toad)$f[2]\nv &lt;- summary(lm_toad)$f[3]\nf2 &lt;- summary(lm_toad)$r.squared/(1-summary(lm_toad)$r.squared)\n  \npwr.f2.test(u, v, f2, sig.level=0.05)\n\n\n     Multiple regression power calculation \n\n              u = 1\n              v = 58\n             f2 = 0.03516689\n      sig.level = 0.05\n          power = 0.297925\n\n\n\n\n\nThese two seeds (chosen specifically to illustrate the point) give wildly different estimates of the statistical power. This is because they also have wildly different R2 values in the model summaries.\nGiven what you know, from the content in this chapter, do you trust the results from either of these single datasets more or less than you trust the results from 1000 datasets combined?\n\n10.6.1 Exercise 2 - Changing power\n\nLevel: \nReturn to the for loop that we used to generate our 1000 datasets, and change some of the parameters.\n\nHow does increasing or decreasing n impact power?\nHow does changing the beta coefficients impact power?\nWhat about changing sdi?\n\nLook at the impact that each of these things has on the set of statistics, p-values, and R2 values that you get.\n\n\n\n10.6.2 Exercise 3 - A priori power analysis\n\nLevel: \nPerhaps the more useful application of power analysis is figuring out how big your sample size needs to be.\nFor those who feel more comfortable with the programming, try running simulations where you vary n over a range, and look at the impact. (Keep all the other parameters constant, unless you’re really looking for a challenge.)\nSee if you can find a sensible cut-off for n where 80% power is achieved.",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulating Datasets</span>"
    ]
  },
  {
    "objectID": "materials/08-simulating-datasets.html#summary",
    "href": "materials/08-simulating-datasets.html#summary",
    "title": "10  Simulating Datasets",
    "section": "10.7 Summary",
    "text": "10.7 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nDatasets can be simulated, by setting up the underlying distribution and sampling randomly from it\nYou can sample from different types of distributions, with varying parameters\nThese simulated datasets can be used for checking your experimental design, and/or testing your analysis pipeline\nSimulations can also be used to perform power analyses",
    "crumbs": [
      "Course Materials",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simulating Datasets</span>"
    ]
  }
]